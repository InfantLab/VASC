{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Actor Synchroncy and Causality (VASC)\n",
    "## RAEng: Measuring Responsive Caregiving Project\n",
    "### Caspar Addyman, 2020\n",
    "### https://github.com/infantlab/VASC\n",
    "\n",
    "# Step 1  Process videos using OpenPose\n",
    "\n",
    "This script uses [OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) human figure recognition neural network to create labeled wireframes for each figure in each frame of a video. OpenPoseDemo will go through a video frame by frame outputing a JSON file for each frame that contains a set of coordinate points and for a wireframe for each video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the python libraries we need\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import json\n",
    "import cv2               #computervision toolkit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "#turn on debugging\n",
    "%pdb on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Where is OpenPose?\n",
    "\n",
    "We need the full path to your openpose directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of openposedemo - THIS WILL BE DIFFERENT ON YOUR COMPUTER\n",
    "# openposepath = \"C:\\\\Users\\\\cas\\\\openpose-1.7.0-binaries-win64-gpu-python3.7-flir-3d_recommended\\\\\"\n",
    "# I've had problems with version 1.7.0 so i'm staying with version 1.5.0 for the moment.\n",
    "openposepath = \"C:\\\\Users\\\\cas\\\\openpose-1.5.0-binaries-win64-gpu-python-flir-3d_recommended\\\\\"\n",
    "\n",
    "if sys.platform == \"win32\":\n",
    "    app = \"bin\\\\OpenPoseDemo.exe\"\n",
    "else:\n",
    "    app = 'bin\\\\OpenPoseDemo.bin'\n",
    "\n",
    "openposeapp = openposepath + app\n",
    "print(openposeapp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Where are your videos?\n",
    "\n",
    "In the next cell you need to specify the folder with your set of video files. So that we process them. These scripts use the following director structure. It expects your videos to be in a subfolder of your project \n",
    "\n",
    "```\n",
    "path\\to\\project\\myvideos\n",
    "```\n",
    "\n",
    "and it expects a folder `out` in the project at the same level as the videos with three subfolders for JSON files, the aggregated timeseries and the analyses. \n",
    "\n",
    "You will need to create all three of these. Or copy them from the VASC project folder.\n",
    "\n",
    "```\n",
    "path\\to\\project\\out\\openpose\n",
    "path\\to\\project\\out\\timeseries\n",
    "path\\to\\project\\out\\analyses\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where's the project data folder? (without trailing slash)\n",
    "projectpath = \"C:\\\\Users\\\\cas\\\\OneDrive - Goldsmiths College\\\\Projects\\\\Measuring Responsive Caregiving\\\\VASCTutorial\"\n",
    "#where are your video files? \n",
    "videos_in = \"C:\\\\Users\\\\cas\\\\OneDrive - Goldsmiths College\\\\Projects\\\\Measuring Responsive Caregiving\\\\VASCTutorial\\\\demovideos\"\n",
    "\n",
    "# locations of videos and output\n",
    "# videos_out   = \"E:\\\\SpeakNSign\\\\\" + \"out\"\n",
    "videos_out = projectpath + \"\\\\out\" \n",
    "videos_out_openpose   = videos_out + \"\\\\openpose\"\n",
    "videos_out_timeseries = videos_out + \"\\\\timeseries\"\n",
    "videos_out_analyses   = videos_out + \"\\\\analyses\"\n",
    "\n",
    "print(videos_in)\n",
    "print(videos_out)\n",
    "print(videos_out_openpose)\n",
    "print(videos_out_timeseries)\n",
    "print(videos_out_analyses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Load the videos \n",
    "\n",
    "We have two ways of loading in videos. Either from an Excel file of names and paths to the videos. Or by scanning contents of a particular folder. But first \n",
    "\n",
    "### 1.4.1 Retrieve or create videos.json\n",
    "\n",
    "We store information about the videos we are processing in a file calle `videos.json`, located at the top level of the `out` folder. \n",
    "\n",
    "So we see if we already have this file. Maybe we've done some processing already.\n",
    "\n",
    "If you want to start afresh just delete this single file and everything gets reprocessed from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve the list of base names of processed videos.\n",
    "try:\n",
    "    with open(videos_out + '\\\\videos.json') as json_file:\n",
    "        videos = json.load(json_file)\n",
    "        print(\"Existing videos.json found..\")\n",
    "except:\n",
    "    videos = {}\n",
    "    print(\"Creating new videos.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EITHER\n",
    "### 1.4.2.A Read an Excel file of videos\n",
    "\n",
    "We expect the first column of the spreadsheet to tell us the base name for each participant and columns 2 to 4 contains the full name and location of the videos.\n",
    "\n",
    "There maybe be multiple camera angles of for same session. \n",
    "\n",
    "```\n",
    "Subject   | camera1      | camera2        | camera3 \n",
    "subj1     | pathtocam1.1 | pathtocam1.2   | pathtocam1.3 \n",
    "subj2     | pathtocam2.1 | pathtocam2.2   | pathtocam3.3 \n",
    "...\n",
    "```\n",
    "\n",
    "If there aren't these columns can be blank in the spreadsheet.\n",
    "\n",
    "```\n",
    "Subject   | camera1      | camera2        | camera3 \n",
    "subj1     | pathtocam1.1 |                |   \n",
    "subj2     | pathtocam2.1 |                |  \n",
    "...\n",
    "```\n",
    "\n",
    "\n",
    "The names come from the spreadsheet so we set a flag `namesfromfiles = False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excelpath = \"U:\\\\Caspar\\\\SS_CARE.xlsx\"\n",
    "videolist = pd.read_excel(excelpath)\n",
    "\n",
    "namesfromfiles = False\n",
    "cameras = [\"camera1\", \"camera2\", \"camera3\"]\n",
    "\n",
    "\n",
    "# iterate through each row and select  \n",
    "# 'Name' and 'Stream' column respectively. \n",
    "for ind in videolist.index :\n",
    "    vid = videolist[\"subject\"][ind]\n",
    "    \n",
    "    if vid in videos:\n",
    "        #we already have some info about this video\n",
    "        print(vid, \"found.\")\n",
    "    else:\n",
    "        #generate an structure to hold some info about this video\n",
    "        videos[vid] = {}\n",
    "    \n",
    "    for cam in cameras:\n",
    "        #do we have a column for this camera in the spreadsheet?\n",
    "        if cam in videolist.columns:\n",
    "            #if so what is path of video it has\n",
    "            fullpath = videolist[cam][ind] \n",
    "            #maybe we processed this already \n",
    "            if cam in videos[vid] and videos[vid][cam][\"fullpath\"] ==  fullpath:\n",
    "                print(cam, videos[vid][cam][\"stemname\"], \"already processed\")\n",
    "                print(\"Exit code:\", videos[vid][cam][\"openpose\"][\"exitcode\"])\n",
    "                print(\"Date:\", videos[vid][cam][\"openpose\"][\"when\"])\n",
    "            else:\n",
    "                fullname = os.path.basename(fullpath) \n",
    "                stemname, fmt = os.path.splitext(fullname)\n",
    "                videos[vid][cam] = {}\n",
    "                videos[vid][cam][\"shortname\"] = vid + \".\" + cam\n",
    "                videos[vid][cam][\"stemname\"] = stemname\n",
    "                videos[vid][cam][\"fullname\"] = fullname\n",
    "                videos[vid][cam][\"fullpath\"] = fullpath\n",
    "                videos[vid][cam][\"index\"] = None         #the numerical index this data will have in np.array.\n",
    "                videos[vid][cam][\"format\"] = fmt\n",
    "                videos[vid][cam][\"openpose\"] =  {\"exitcode\" : None, \"when\" : None} \n",
    "\n",
    "    \n",
    "print(videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or \n",
    "### 1.4.3.B scanning all videos in particular folder \n",
    "\n",
    "In which case we look at all videos in `videos_in` and it's subfolders. \n",
    "\n",
    "We let the names of the files also provide the base names for each participant we create.  \n",
    "\n",
    "We will reference these files by the video names in our `videos.json` data structure. So myvid1.avi is found in `videos[\"myvid1\"]`.\n",
    "\n",
    "However, in other cases we will allow for possibility of multiple camera angles so this defaults to `\"camera1\"`.\n",
    "\n",
    "We set a flag `namesfromfiles = True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first get list of videos in the video folder and subfolders.\n",
    "avis =     glob.glob(videos_in + \"/**/*.avi\", recursive = True)\n",
    "mp4s =     glob.glob(videos_in + \"/**/*.mp4\", recursive = True)\n",
    "threegps = glob.glob(videos_in + \"/**/*.3gp\", recursive = True)\n",
    "\n",
    "print(\"We found %d avis\" % len(avis))\n",
    "print(\"We found %d mp4s\" % len(mp4s))\n",
    "print(\"We found %d 3gps\" % len(threegps))\n",
    "\n",
    "#For the moment we will manually specify what videos to process. \n",
    "#TODO generate a list of force or skip videos to automate things slightly\n",
    "allvideos = []\n",
    "allvideos.extend(avis)\n",
    "allvideos.extend(mp4s)\n",
    "allvideos.extend(threegps)\n",
    "\n",
    "namesfromfiles = True\n",
    "\n",
    "\n",
    "for thisvid in allvideos:\n",
    "    #first we need base name of video for the output file name\n",
    "    #we will reference these files by the video names so myvid1.avi is found in videos[\"myvid1\"]0\n",
    "    fullname = os.path.basename(thisvid)\n",
    "    vid, fmt = os.path.splitext(fullname) \n",
    "    #generate an structure to hold some info about this video\n",
    "    if vid in videos: \n",
    "        print(vid + \" already in videos.json\")\n",
    "    else:\n",
    "        print(\"Adding \" + vid + \" to videos.json\")\n",
    "        videos[vid] = {}  \n",
    "        cam = \"camera1\"\n",
    "        videos[vid][cam] = {} \n",
    "        videos[vid][cam][\"shortname\"] = vid + \".\" + cam\n",
    "        videos[vid][cam][\"stemname\"] = vid\n",
    "        videos[vid][cam][\"fullname\"] = fullname\n",
    "        videos[vid][cam][\"fullpath\"] = thisvid\n",
    "        videos[vid][cam][\"index\"] = None         #the numerical index this data will have in np.array.\n",
    "        videos[vid][cam][\"format\"] = fmt\n",
    "        videos[vid][cam][\"openpose\"] =  {\"exitcode\" : None, \"when\" : None} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Calling the OpenPose app\n",
    "To operate OpenPose we pass a set of parameters to the demo executable. For the full list of options see  [OpenPoseDemo](https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/demo_overview.md)\n",
    "\n",
    "Our main parameters are\n",
    "\n",
    "```\n",
    "--video        path\\to\\video_to_process   #input video\n",
    "--write_json   path\\to\\output_directory   #one json file per frame\n",
    "--write_video  path\\to\\output_directory   #video with identified figures\n",
    "--write_images path\\to\\output_directory   #one image per frame with wireframes\n",
    "--disable_blending true/false             # wireframes on black background (true) or blended on top of video (false)\n",
    " ```\n",
    "\n",
    "Other useful params\n",
    " ```\n",
    "--frame_first  100    #start from frame 100\n",
    "--display 0           #don't show the images as they are processed\n",
    " ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put all params in a dictionary object\n",
    "params = dict()\n",
    "params[\"write_json\"] = videos_out_openpose\n",
    "# params[\"write_images\"] = videos_out_openpose  #for the moment dump images in output file - TODO name subfolder\n",
    "#params[\"disable_blending\"] = \"false\"\n",
    "params[\"display\"]  = \"1\"\n",
    "\n",
    "createoutputvideo = True #do we get openpose to create a video output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The main openpose loop\n",
    "\n",
    "Call the `openposedemo` app for each of the videos at a time. For each one print the full command that we use so that you can use it manually to investigate any errors. \n",
    "\n",
    "Finally, we write a list of the processed videos to a file called `videos.json`. \n",
    "Note that we will add other information to this file as we go through other steps. \n",
    "\n",
    "TODO - We might use videos.json to let this processing happen in blocks. i.e. not calling for openpose if video is already processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currdir =  os.getcwd() + \"\\\\\" #keep track of current directory so we can change back to it after processing\n",
    "\n",
    "optstring = \"\"\n",
    "for key in params:\n",
    "    optstring += \" --\" + key +  ' \"' + params[key] + '\"' #need to quote paths \n",
    "\n",
    "print(optstring)\n",
    "\n",
    "\n",
    "count = 0\n",
    "os.chdir(openposepath)\n",
    "for vid in videos:\n",
    "    #first we need base name of video for the output file name\n",
    "    video_outname = vid + \"_output.avi\"\n",
    "    for cam in videos[vid]:\n",
    "        print(\"\\n\\nStaring openpose processing of \" + vid + \".\" + cam )\n",
    "        if videos[vid][cam][\"openpose\"][\"exitcode\"] == 0:\n",
    "            print(\"Already processed\", videos[vid][cam][\"openpose\"][\"when\"] )\n",
    "        else:\n",
    "            try:\n",
    "                # Log the time\n",
    "                time_start = time.time()\n",
    "                video = ' --video \"' + videos[vid][cam][\"fullpath\"] + '\"'\n",
    "                if createoutputvideo:\n",
    "                    video_out = ' --write_video \"' + videos_out_openpose + '\\\\' + vid + cam + \"_output.avi\" + '\"'\n",
    "                else:\n",
    "                    video_out = \"\"\n",
    "                openposecommand = openposeapp + video + video_out + optstring\n",
    "                print(openposecommand)\n",
    "                exitcode = os.system(openposecommand)\n",
    "                videos[vid][cam][\"openpose\"][\"exitcode\"] = exitcode\n",
    "                # Log the time again\n",
    "                time_end = time.time()\n",
    "                if (exitcode == 0):\n",
    "                    videos[vid][cam][\"index\"] = count  #TODO - Use this \n",
    "                    count += 1\n",
    "                    videos[vid][cam][\"openpose\"][\"exitcode\"] = 0\n",
    "                    videos[vid][cam][\"openpose\"][\"when\"] = datetime.now().isoformat()\n",
    "                    videos[vid][cam][\"openpose\"][\"out\"] = videos_out_openpose + '\\\\' + video_outname\n",
    "                    print (\"Done \" + vid + cam)\n",
    "                    print (\"It took %d seconds for conversion.\" % (time_end-time_start))\n",
    "                else:\n",
    "                    print(\"OpenPose error. Exit code %d\" % exitcode)\n",
    "                    videos[vid][cam][\"openpose\"][\"exitcode\"] = exitcode\n",
    "                    videos[vid][cam][\"openpose\"][\"when\"] = datetime.now().isoformat()\n",
    "            except Exception as e:\n",
    "                print(\"Error: \", e)\n",
    "                pass\n",
    "    #after each new id we save the json data\n",
    "    with open(videos_out + '\\\\videos.json', 'w') as outfile:\n",
    "        json.dump(videos, outfile)\n",
    "        print('videos.json updated')\n",
    "\n",
    "        \n",
    "#change the directory back\n",
    "os.chdir(currdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Gather the data into useable format.\n",
    "\n",
    "OpenPose has created one JSON file per frame of video. We want to group these up into bigger arrays. \n",
    "\n",
    "This routine needs to know where to find the processed videos and what are the base names. These are listed in the `videos.json` file we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve the list of base names of processed videos.\n",
    "with open(videos_out + '\\\\videos.json') as json_file:\n",
    "    videos = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First find out the height, width and frames per second for each video and add this to `videos.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a note of some video properties\n",
    "\n",
    "for vid in videos:\n",
    "    for cam in videos[vid]:\n",
    "        cap = cv2.VideoCapture(videos[vid][cam][\"fullpath\"]) # 0=camera\n",
    "        if cap.isOpened(): \n",
    "            videos[vid][cam][\"height\"] = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "            videos[vid][cam][\"width\"] = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "            videos[vid][cam][\"fps\"] = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "            videos[vid][cam][\"n_frames\"] = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional\n",
    "#print these out to remind ourselves. \n",
    "for vid in videos:  \n",
    "    print(vid, \":\")\n",
    "    for cam in videos[vid]:\n",
    "        print(videos[vid][cam])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Extracting all the numeric data from the json files\n",
    "\n",
    "We loop through the list of names in `videos` and search for all json files associated with that name. We then extract all the coordinates and confidence scores for all identified people in each frame and store them in one big multidimensional padded array.\n",
    "\n",
    "```\n",
    "1st dimension - number of videos\n",
    "2nd dimension - max number of cameras (often 1 but sometimes 2 or 3) \n",
    "3rd dimension - max nummber of frames\n",
    "4th dimension - max number of people\n",
    "5th dimension - number of values (per person) output by openpose\n",
    "```\n",
    "\n",
    "For example, if we had the following videos \n",
    "\n",
    "```\n",
    "video1 - cam1 - 200 frames  - 3 people (max) \n",
    "video1 - cam2 - 200 frames  - 3 people (max) \n",
    "video2 - 203 frames  - 2 people (max) \n",
    "video3 - 219 frames  - 4 people (max) \n",
    "```\n",
    "\n",
    "then we'd create a `3 x 2 x 219 x 4 x 75` array.\n",
    "\n",
    "First we see how big the dimensions of the array have to be. \n",
    "And create an numpy array called `keypoints_array` big enough to hold all of this.\n",
    "\n",
    "As a sanity check we count the number of frames processed by openpose. Ought to be same as above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvideos = len(videos)\n",
    "maxcameras = 3\n",
    "maxframes = 0\n",
    "maxpeople = 15 #maximum people we might expect (large upper bound)\n",
    "ncoords = 75 #the length of the array coming back from openpose x,y coords of each point plus pafs\n",
    "\n",
    "for vid in videos:    \n",
    "    for cam in videos[vid]:    \n",
    "        #use glob to get all the individual json files.\n",
    "        videoname = videos[vid][cam][\"stemname\"]\n",
    "        alljson = glob.glob(videos_out_openpose + \"\\\\\" + videoname + \"*.json\")\n",
    "        nframes = len(alljson)\n",
    "        print(\"Video\", vid, cam, \"has {0} frames.\".format(nframes))\n",
    "        videos[vid][cam][\"frames\"] = nframes\n",
    "        maxframes = max(maxframes,nframes)\n",
    "    \n",
    "    \n",
    "keypoints_array = np.zeros([nvideos,maxcameras, maxframes,maxpeople,ncoords]) #big array to hold all the numbers\n",
    "print(\"Initialise numpy array of size\", keypoints_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now loop through all the videos copying the frame data into our big `keypoints_array` and also seeing how many people (max) are detected in each one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npeople = np.zeros(maxframes)  #an array to track how many people detected per frame.\n",
    "globalmaxpeople =  0\n",
    "v = -1\n",
    "\n",
    "for vid in videos: \n",
    "    v += 1  #index for this subject\n",
    "    c = -1\n",
    "    for cam in videos[vid]:\n",
    "        c += 1  #index for this camera\n",
    "        #use glob to get all the individual json files.\n",
    "        alljson = glob.glob(videos_out_openpose + \"\\\\\" + videos[vid][cam][\"stemname\"] + \"*.json\") \n",
    "        i = 0\n",
    "        for frame in alljson:\n",
    "            with open(frame, \"r\") as read_file:\n",
    "                data = json.load(read_file)\n",
    "                j = 0\n",
    "                for p in data[\"people\"]:\n",
    "                    keypoints = p[\"pose_keypoints_2d\"]  \n",
    "                    keypoints_array[v,c,i,j,:]=keypoints\n",
    "                    j += 1\n",
    "                npeople[i] = j\n",
    "                i += 1\n",
    "        #end loop for this video\n",
    "        people = int(max(npeople))\n",
    "        print(\"Video\", vid, cam, \"has {0} people detected.\".format(people))\n",
    "        videos[vid][cam][\"maxpeople\"] = people\n",
    "        videos[vid][cam][\"v\"] = v  #might be useful to have these indices available\n",
    "        videos[vid][cam][\"c\"] = c\n",
    "        #how many people did it contain? Is this biggest number so far?\n",
    "        globalmaxpeople = max(globalmaxpeople, people)\n",
    "        \n",
    "    \n",
    "#and just like that n videos have been reduced to a big block of people coords.\n",
    "#we now truncate the array for the maximum number of people as the rest of it is all zeros\n",
    "\n",
    "keypoints_array = np.delete(keypoints_array,np.s_[int(globalmaxpeople):],3)\n",
    "\n",
    "print(\"keypoints_array has size\", keypoints_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Save the data!\n",
    "\n",
    "Saving the data at this stage so we don't have to repeat these steps again if we reorganise or reanalyse the data.\n",
    "\n",
    "We create a compressed NumPy array `allframedata.npz` containing the person location data for all the videos. \n",
    "\n",
    "We also update the `videos.json` file with more info about the videos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "#update the json file in the video out directory\n",
    "with open(videos_out + '\\\\videos.json', 'w') as outfile:\n",
    "    json.dump(videos, outfile)\n",
    "\n",
    "# in the time series folder we save the data file. \n",
    "#in a compressed format as it has a lot of empty values\n",
    "np.savez_compressed(videos_out_timeseries + '\\\\allframedata.npz', keypoints_array=keypoints_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### That's it. \n",
    "\n",
    "Now go onto [Step 2 - Organising the data](Step2.OrganiseData.ipynb)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
