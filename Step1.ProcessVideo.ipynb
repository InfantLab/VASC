{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Actor Synchroncy and Causality (VASC)\n",
    "\n",
    "## RAEng: Measuring Responsive Caregiving Project\n",
    "### Caspar Addyman, 2020\n",
    "### https://github.com/infantlab/VASC\n",
    "\n",
    "# Step 1  Process videos using OpenPose\n",
    "\n",
    "This script uses [OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) human figure recognition neural network to create labeled wireframes for each figure in each frame of a video. OpenPoseDemo will go through a video frame by frame outputing a JSON file for each frame that contains a set of coordinate points and for a wireframe for each video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 - Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "#import the python libraries we need\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import json\n",
    "import cv2               #computervision toolkit\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "#turn on debugging & logging\n",
    "%pdb on\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Settings?\n",
    "\n",
    "Load a json file that tells us where to find our videos and where to save the data.  You should create a different settings file for each project. Then you don't need to change any other values in the script for Step 1 or Step 2.\n",
    "\n",
    "\n",
    "`TODO - write a helper to create a settings file`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing settings.json found..\n"
     ]
    }
   ],
   "source": [
    "settingsjson = \".\\\\Drum.Tutorial.settings.json\"\n",
    "\n",
    "try:\n",
    "    with open(settingsjson) as json_file:\n",
    "        settings = json.load(json_file)\n",
    "        print(\"Existing settings.json found..\")\n",
    "except json.JSONDecodeError:\n",
    "    logging.exception(\"Settings file was not valid JSON.\")\n",
    "except Exception as e:\n",
    "        emsg = str(e)\n",
    "        #show the error\n",
    "        print(\"Error: \",emsg)\n",
    "        print(\"No setting.json file found!\\nPlease see Step 0 for instructions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batchName': 'VASC Drumming Data tutorial',\n",
      " 'filenames': {'alldatanpz': 'allframedata.npz',\n",
      "               'clean_json': 'clean.json',\n",
      "               'cleandataparquet': 'cleandata.parquet',\n",
      "               'cleanleftnpz': 'cleanleftdata.npz',\n",
      "               'cleannpz': 'cleandata.npz',\n",
      "               'cleanrightnpz': 'cleanrightdata.npz',\n",
      "               'lefthandnpz': 'lefthandframedata.npz',\n",
      "               'lefthandparquet': 'lefthand.parquet',\n",
      "               'righthandnpz': 'righthandframedata.npz',\n",
      "               'righthandparquet': 'righthand.parquet',\n",
      "               'videos_json': 'videos.json'},\n",
      " 'flags': {'anon': False, 'cleaned': False, 'includeHands': True},\n",
      " 'lastUpdate': None,\n",
      " 'paths': {'openpose': 'C:\\\\Users\\\\cas\\\\openpose-1.5.0-binaries-win64-gpu-python-flir-3d_recommended\\\\',\n",
      "           'project': '.\\\\DrumTutorial',\n",
      "           'videos_in': '.\\\\DrumTutorial\\\\videos',\n",
      "           'videos_out': '.\\\\DrumTutorial\\\\',\n",
      "           'videos_out_analyses': '.\\\\DrumTutorial\\\\analyses',\n",
      "           'videos_out_openpose': '.\\\\DrumTutorial\\\\openpose',\n",
      "           'videos_out_timeseries': '.\\\\DrumTutorial\\\\timeseries'}}\n"
     ]
    }
   ],
   "source": [
    "#optional - have a look at our settings to make sure they're what we expect\n",
    "pprint(settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Where is OpenPose?\n",
    "\n",
    "We need the full path to your openpose directory (this is in settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cas\\openpose-1.5.0-binaries-win64-gpu-python-flir-3d_recommended\\bin\\OpenPoseDemo.exe\n"
     ]
    }
   ],
   "source": [
    "# location of openposedemo - THIS WILL BE DIFFERENT ON YOUR COMPUTER\n",
    "openposepath = settings[\"paths\"][\"openpose\"]\n",
    "\n",
    "if sys.platform == \"win32\":\n",
    "    app = \"bin\\\\OpenPoseDemo.exe\"\n",
    "else:\n",
    "    app = 'bin\\\\OpenPoseDemo.bin'\n",
    "\n",
    "openposeapp = openposepath + app\n",
    "print(openposeapp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.3 Where are your videos?\n",
    "\n",
    "In the next cell you need to specify the folder with your set of video files. So that we process them. These scripts use the following directory structure. It expects your videos to be in a subfolder of your project\n",
    "\n",
    "```\n",
    "path\\to\\project\\myvideos\n",
    "```\n",
    "\n",
    "and it expects a folder `out` in the project at the same level as the videos with three subfolders for JSON files, the aggregated timeseries and the analyses.\n",
    "\n",
    "You will need to create all three of these. Or copy them from the VASC project folder.\n",
    "\n",
    "```\n",
    "path\\to\\project\\out\\openpose\n",
    "path\\to\\project\\out\\timeseries\n",
    "path\\to\\project\\out\\analyses\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "videos_in:  .\\DrumTutorial\\videos\n",
      "videos_out:  .\\DrumTutorial\\\n",
      "videos_out_openpose:  .\\DrumTutorial\\openpose\n",
      "videos_out_timeseries:  .\\DrumTutorial\\timeseries\n",
      "videos_out_analyses:  .\\DrumTutorial\\analyses\n"
     ]
    }
   ],
   "source": [
    "# where's the project data folder? (with trailing slash)\n",
    "projectpath = settings[\"paths\"][\"project\"]\n",
    "#where are your video files?\n",
    "videos_in = settings[\"paths\"][\"videos_in\"]\n",
    "\n",
    "# locations of videos and output\n",
    "videos_out = settings[\"paths\"][\"videos_out\"]\n",
    "videos_out_openpose   = settings[\"paths\"][\"videos_out_openpose\"]\n",
    "videos_out_timeseries = settings[\"paths\"][\"videos_out_timeseries\"]\n",
    "videos_out_analyses   = settings[\"paths\"][\"videos_out_analyses\"]\n",
    "\n",
    "print(\"videos_in: \", videos_in)\n",
    "print(\"videos_out: \", videos_out)\n",
    "print(\"videos_out_openpose: \", videos_out_openpose)\n",
    "print(\"videos_out_timeseries: \", videos_out_timeseries)\n",
    "print(\"videos_out_analyses: \", videos_out_analyses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Load the videos\n",
    "\n",
    "We have two ways of loading in videos. Either from an Excel file of names and paths to the videos. Or by scanning contents of a particular folder. But first\n",
    "\n",
    "### 1.4.1 Retrieve or create videos.json\n",
    "\n",
    "We store information about the videos we are processing in a file calle `videos.json`, located at the top level of the `out` folder.\n",
    "\n",
    "So we see if we already have this file. Maybe we've done some processing already.\n",
    "\n",
    "If you want to start afresh just delete this single file and everything gets reprocessed from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing videos.json found..\n"
     ]
    }
   ],
   "source": [
    "#retrieve the list of base names of processed videos.\n",
    "videosjson = os.path.abspath(settings[\"paths\"][\"videos_out\"] + '\\\\' + settings[\"filenames\"][\"videos_json\"])\n",
    "try:\n",
    "    with open(videosjson) as json_file:\n",
    "        videos = json.load(json_file)\n",
    "        print(\"Existing videos.json found..\")\n",
    "except:\n",
    "    videos = {}\n",
    "    with open(videosjson, 'w') as outfile:\n",
    "        json.dump(videos, outfile)\n",
    "        print(\"Creating new videos.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Either\n",
    "### 1.4.3.A Scanning all videos in particular folder\n",
    "\n",
    "In which case we look at all videos in `videos_in` let the names of the files also provide the base names for each participant we create.\n",
    "\n",
    "We will reference these files by the video names so myvid1.avi is found in `videos[\"myvid1\"]`.\n",
    "\n",
    "However, in other cases we will allow for possibility of multiple camera angles so this defaults to `\"camera1\"`.\n",
    "\n",
    "We set a flag `namesfromfiles = True`.\n",
    "\n",
    "If your vidoes aren't appearing there might be too many or too few trailing slashes in your path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching .\\DrumTutorial\\videos\n",
      "We found 30 mp4s\n"
     ]
    }
   ],
   "source": [
    "#quick check to see if we can find videos\n",
    "mp4s =     glob.glob(videos_in + \"*/*.mp4\", recursive = True)\n",
    "print(f\"Searching {videos_in}\")\n",
    "print(f\"We found {len(mp4s)} mp4s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found 0 avis\n",
      "We found 30 mp4s\n",
      "We found 0 3gps\n",
      "1fa339b_04-test-trials already in videos.json\n",
      "1fa339b_06-test-trials already in videos.json\n",
      "1fa339b_08-test-trials already in videos.json\n",
      "1fa339b_10-test-trials already in videos.json\n",
      "1fa339b_12-test-trials already in videos.json\n",
      "1fa339b_14-test-trials already in videos.json\n",
      "768fa72_04-test-trials already in videos.json\n",
      "768fa72_06-test-trials already in videos.json\n",
      "768fa72_08-test-trials already in videos.json\n",
      "768fa72_10-test-trials already in videos.json\n",
      "768fa72_12-test-trials already in videos.json\n",
      "768fa72_14-test-trials already in videos.json\n",
      "b22644a_04-test-trials already in videos.json\n",
      "b22644a_06-test-trials already in videos.json\n",
      "b22644a_08-test-trials already in videos.json\n",
      "b22644a_10-test-trials already in videos.json\n",
      "b22644a_12-test-trials already in videos.json\n",
      "b22644a_14-test-trials already in videos.json\n",
      "CasparA_04-test-trials already in videos.json\n",
      "CasparA_06-test-trials already in videos.json\n",
      "CasparA_08-test-trials already in videos.json\n",
      "CasparA_10-test-trials already in videos.json\n",
      "CasparA_12-test-trials already in videos.json\n",
      "CasparA_14-test-trials already in videos.json\n",
      "SineadR_04-test-trials already in videos.json\n",
      "SineadR_06-test-trials already in videos.json\n",
      "SineadR_08-test-trials already in videos.json\n",
      "SineadR_10-test-trials already in videos.json\n",
      "SineadR_12-test-trials already in videos.json\n",
      "SineadR_14-test-trials already in videos.json\n"
     ]
    }
   ],
   "source": [
    "#first get list of videos in the inbox\n",
    "avis =     glob.glob(videos_in + \"*/*.avi\", recursive = True)\n",
    "mp4s =     glob.glob(videos_in + \"*/*.mp4\", recursive = True)\n",
    "threegps = glob.glob(videos_in + \"*/*.3gp\", recursive = True)\n",
    "\n",
    "print(\"We found %d avis\" % len(avis))\n",
    "print(\"We found %d mp4s\" % len(mp4s))\n",
    "print(\"We found %d 3gps\" % len(threegps))\n",
    "\n",
    "#For the moment we will manually specify what videos to process.\n",
    "#TODO generate a list of force or skip videos to automate things slightly\n",
    "allvideos = []\n",
    "allvideos.extend(avis)\n",
    "allvideos.extend(mp4s)\n",
    "allvideos.extend(threegps)\n",
    "\n",
    "namesfromfiles = True\n",
    "\n",
    "\n",
    "for thisvid in allvideos:\n",
    "    #first we need base name of video for the output file name\n",
    "    #we will reference these files by the video names so myvid1.avi is found in videos[\"myvid1\"]0\n",
    "    fullname = os.path.basename(thisvid)\n",
    "    fullpath = str(os.path.abspath(thisvid))\n",
    "    vid, fmt = os.path.splitext(fullname)\n",
    "    #generate an structure to hold some info about this video\n",
    "    if vid in videos:\n",
    "        print(vid + \" already in videos.json\")\n",
    "    else:\n",
    "        print(\"Adding \" + vid + \" to videos.json\")\n",
    "        videos[vid] = {}\n",
    "        cam = \"camera1\"\n",
    "        videos[vid][cam] = {}\n",
    "        videos[vid][cam][\"shortname\"] = vid + \".\" + cam\n",
    "        videos[vid][cam][\"stemname\"] = vid\n",
    "        videos[vid][cam][\"fullname\"] = fullname\n",
    "        videos[vid][cam][\"fullpath\"] = fullpath\n",
    "        videos[vid][cam][\"index\"] = None         #the numerical index this data will have in np.array.\n",
    "        videos[vid][cam][\"format\"] = fmt\n",
    "        videos[vid][cam][\"openpose\"] =  {\"exitcode\" : None, \"when\" : None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.abspath(thisvid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OR\n",
    "### 1.4.2.B Read an Excel file of videos\n",
    "\n",
    "We expect the first column of the spreadsheet to tell us the base name for each participant and columns 2 to 4 contains the full name and location of the videos.\n",
    "\n",
    "There maybe be multiple camera angles of for same session.\n",
    "\n",
    "```\n",
    "Subject   | camera1      | camera2        | camera3\n",
    "subj1     | pathtocam1.1 | pathtocam1.2   | pathtocam1.3\n",
    "subj2     | pathtocam2.1 | pathtocam2.2   | pathtocam3.3\n",
    "...\n",
    "```\n",
    "\n",
    "If there aren't these columns can be blank in the spreadsheet.\n",
    "\n",
    "```\n",
    "Subject   | camera1      | camera2        | camera3\n",
    "subj1     | pathtocam1.1 |                |\n",
    "subj2     | pathtocam2.1 |                |\n",
    "...\n",
    "```\n",
    "\n",
    "\n",
    "The names come from the spreadsheet so we set a flag `namesfromfiles = False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excelpath = \"U:\\\\Caspar\\\\SS_CARE.xlsx\"\n",
    "videolist = pd.read_excel(excelpath)\n",
    "\n",
    "namesfromfiles = False\n",
    "cameras = [\"camera1\", \"camera2\", \"camera3\"]\n",
    "\n",
    "\n",
    "# iterate through each row and select\n",
    "# 'Name' and 'Stream' column respectively.\n",
    "for ind in videolist.index :\n",
    "    vid = videolist[\"subject\"][ind]\n",
    "\n",
    "    if vid in videos:\n",
    "        #we already have some info about this video\n",
    "        print(vid, \"found.\")\n",
    "    else:\n",
    "        #generate an structure to hold some info about this video\n",
    "        videos[vid] = {}\n",
    "\n",
    "    for cam in cameras:\n",
    "        #do we have a column for this camera in the spreadsheet?\n",
    "        if cam in videolist.columns:\n",
    "            #if so what is path of video it has\n",
    "            fullpath = os.path.abspath(videolist[cam][ind])\n",
    "            #maybe we processed this already\n",
    "            if cam in videos[vid] and videos[vid][cam][\"fullpath\"] ==  fullpath:\n",
    "                print(cam, videos[vid][cam][\"stemname\"], \"already processed\")\n",
    "                print(\"Exit code:\", videos[vid][cam][\"openpose\"][\"exitcode\"])\n",
    "                print(\"Date:\", videos[vid][cam][\"openpose\"][\"when\"])\n",
    "            else:\n",
    "                fullname = os.path.basename(fullpath)\n",
    "                stemname, fmt = os.path.splitext(fullname)\n",
    "                videos[vid][cam] = {}\n",
    "                videos[vid][cam][\"shortname\"] = vid + \".\" + cam\n",
    "                videos[vid][cam][\"stemname\"] = stemname\n",
    "                videos[vid][cam][\"fullname\"] = fullname\n",
    "                videos[vid][cam][\"fullpath\"] = fullpath\n",
    "                videos[vid][cam][\"index\"] = None         #the numerical index this data will have in np.array.\n",
    "                videos[vid][cam][\"format\"] = fmt\n",
    "                videos[vid][cam][\"openpose\"] =  {\"exitcode\" : None, \"when\" : None}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.3 Review list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(videos)\n",
    "#beware that pretty print sometimes messes up the display formating of fullpath,.\n",
    "#(this should not affect actual code that uses fullpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Calling the OpenPose app\n",
    "To operate OpenPose we pass a set of parameters to the demo executable. For the full list of options see  [OpenPoseDemo](https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/demo_overview.md)\n",
    "\n",
    "Our main parameters are\n",
    "\n",
    "```\n",
    "--video        path\\to\\video_to_process   #input video\n",
    "--write_json   path\\to\\output_directory   #one json file per frame\n",
    "--write_video  path\\to\\output_directory   #video with identified figures\n",
    "--write_images path\\to\\output_directory   #one image per frame with wireframes\n",
    "--disable_blending true/false             # wireframes on black background (true) or blended on top of video (false)\n",
    " ```\n",
    "\n",
    "Other useful params\n",
    " ```\n",
    "--hand                #include a model and points for the hands.\n",
    "--frame_first  100    #start from frame 100\n",
    "--display 0           #don't show the images as they are processed\n",
    " ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#should we generate and process points for hand data?\n",
    "includeHands = settings[\"flags\"][\"includeHands\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put all params in a dictionary object\n",
    "params = dict()\n",
    "params[\"write_json\"] = os.path.abspath(videos_out_openpose)\n",
    "# params[\"write_images\"] = videos_out_openpose  #for the moment dump images in output file - TODO name subfolder\n",
    "#params[\"disable_blending\"] = \"false\"\n",
    "params[\"display\"]  = \"1\"\n",
    "if includeHands:\n",
    "    params[\"hand\"] = \"\"\n",
    "\n",
    "createoutputvideo = True #do we get openpose to create a video output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The main openpose loop\n",
    "\n",
    "Call the `openposedemo` app for each of the videos at a time. For each one print the full command that we use so that you can use it manually to investigate any errors.\n",
    "\n",
    "Finally, we write a list of the processed videos to a file called `videos.json`.\n",
    "Note that we will add other information to this file as we go through other steps.\n",
    "\n",
    "TODO - We might use videos.json to let this processing happen in blocks. i.e. not calling for openpose if video is already processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --write_json \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\" --display \"1\" --hand \"\"\n",
      "\n",
      "\n",
      "Staring openpose processing of 1fa339b_04-test-trials.camera1\n",
      "C:\\Users\\cas\\openpose-1.5.0-binaries-win64-gpu-python-flir-3d_recommended\\bin\\OpenPoseDemo.exe --video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\videos\\1fa339b_04-test-trials.mp4\" --write_video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\\1fa339b_04-test-trialscamera1_output.avi\" --write_json \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\" --display \"1\" --hand \"\"\n",
      "Done 1fa339b_04-test-trialscamera1\n",
      "It took 230 seconds for conversion.\n",
      "videos.json updated\n",
      "\n",
      "\n",
      "Staring openpose processing of 1fa339b_06-test-trials.camera1\n",
      "C:\\Users\\cas\\openpose-1.5.0-binaries-win64-gpu-python-flir-3d_recommended\\bin\\OpenPoseDemo.exe --video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\videos\\1fa339b_06-test-trials.mp4\" --write_video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\\1fa339b_06-test-trialscamera1_output.avi\" --write_json \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\" --display \"1\" --hand \"\"\n",
      "Done 1fa339b_06-test-trialscamera1\n",
      "It took 137 seconds for conversion.\n",
      "videos.json updated\n",
      "\n",
      "\n",
      "Staring openpose processing of 1fa339b_08-test-trials.camera1\n",
      "C:\\Users\\cas\\openpose-1.5.0-binaries-win64-gpu-python-flir-3d_recommended\\bin\\OpenPoseDemo.exe --video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\videos\\1fa339b_08-test-trials.mp4\" --write_video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\\1fa339b_08-test-trialscamera1_output.avi\" --write_json \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\" --display \"1\" --hand \"\"\n",
      "Done 1fa339b_08-test-trialscamera1\n",
      "It took 141 seconds for conversion.\n",
      "videos.json updated\n",
      "\n",
      "\n",
      "Staring openpose processing of 1fa339b_10-test-trials.camera1\n",
      "C:\\Users\\cas\\openpose-1.5.0-binaries-win64-gpu-python-flir-3d_recommended\\bin\\OpenPoseDemo.exe --video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\videos\\1fa339b_10-test-trials.mp4\" --write_video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\\1fa339b_10-test-trialscamera1_output.avi\" --write_json \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\" --display \"1\" --hand \"\"\n",
      "Done 1fa339b_10-test-trialscamera1\n",
      "It took 149 seconds for conversion.\n",
      "videos.json updated\n",
      "\n",
      "\n",
      "Staring openpose processing of 1fa339b_12-test-trials.camera1\n",
      "C:\\Users\\cas\\openpose-1.5.0-binaries-win64-gpu-python-flir-3d_recommended\\bin\\OpenPoseDemo.exe --video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\videos\\1fa339b_12-test-trials.mp4\" --write_video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\\1fa339b_12-test-trialscamera1_output.avi\" --write_json \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\" --display \"1\" --hand \"\"\n",
      "Done 1fa339b_12-test-trialscamera1\n",
      "It took 169 seconds for conversion.\n",
      "videos.json updated\n",
      "\n",
      "\n",
      "Staring openpose processing of 1fa339b_14-test-trials.camera1\n",
      "C:\\Users\\cas\\openpose-1.5.0-binaries-win64-gpu-python-flir-3d_recommended\\bin\\OpenPoseDemo.exe --video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\videos\\1fa339b_14-test-trials.mp4\" --write_video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\\1fa339b_14-test-trialscamera1_output.avi\" --write_json \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\" --display \"1\" --hand \"\"\n",
      "Done 1fa339b_14-test-trialscamera1\n",
      "It took 186 seconds for conversion.\n",
      "videos.json updated\n",
      "\n",
      "\n",
      "Staring openpose processing of 768fa72_04-test-trials.camera1\n",
      "C:\\Users\\cas\\openpose-1.5.0-binaries-win64-gpu-python-flir-3d_recommended\\bin\\OpenPoseDemo.exe --video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\videos\\768fa72_04-test-trials.mp4\" --write_video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\\768fa72_04-test-trialscamera1_output.avi\" --write_json \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\" --display \"1\" --hand \"\"\n",
      "Done 768fa72_04-test-trialscamera1\n",
      "It took 219 seconds for conversion.\n",
      "videos.json updated\n",
      "\n",
      "\n",
      "Staring openpose processing of 768fa72_06-test-trials.camera1\n",
      "C:\\Users\\cas\\openpose-1.5.0-binaries-win64-gpu-python-flir-3d_recommended\\bin\\OpenPoseDemo.exe --video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\videos\\768fa72_06-test-trials.mp4\" --write_video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\\768fa72_06-test-trialscamera1_output.avi\" --write_json \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\" --display \"1\" --hand \"\"\n",
      "Done 768fa72_06-test-trialscamera1\n",
      "It took 145 seconds for conversion.\n",
      "videos.json updated\n",
      "\n",
      "\n",
      "Staring openpose processing of 768fa72_08-test-trials.camera1\n",
      "C:\\Users\\cas\\openpose-1.5.0-binaries-win64-gpu-python-flir-3d_recommended\\bin\\OpenPoseDemo.exe --video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\videos\\768fa72_08-test-trials.mp4\" --write_video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\\768fa72_08-test-trialscamera1_output.avi\" --write_json \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\" --display \"1\" --hand \"\"\n",
      "Done 768fa72_08-test-trialscamera1\n",
      "It took 150 seconds for conversion.\n",
      "videos.json updated\n",
      "\n",
      "\n",
      "Staring openpose processing of 768fa72_10-test-trials.camera1\n",
      "C:\\Users\\cas\\openpose-1.5.0-binaries-win64-gpu-python-flir-3d_recommended\\bin\\OpenPoseDemo.exe --video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\videos\\768fa72_10-test-trials.mp4\" --write_video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\\768fa72_10-test-trialscamera1_output.avi\" --write_json \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\" --display \"1\" --hand \"\"\n",
      "Done 768fa72_10-test-trialscamera1\n",
      "It took 156 seconds for conversion.\n",
      "videos.json updated\n",
      "\n",
      "\n",
      "Staring openpose processing of 768fa72_12-test-trials.camera1\n",
      "C:\\Users\\cas\\openpose-1.5.0-binaries-win64-gpu-python-flir-3d_recommended\\bin\\OpenPoseDemo.exe --video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\videos\\768fa72_12-test-trials.mp4\" --write_video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\\768fa72_12-test-trialscamera1_output.avi\" --write_json \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\" --display \"1\" --hand \"\"\n",
      "Done 768fa72_12-test-trialscamera1\n",
      "It took 162 seconds for conversion.\n",
      "videos.json updated\n",
      "\n",
      "\n",
      "Staring openpose processing of 768fa72_14-test-trials.camera1\n",
      "C:\\Users\\cas\\openpose-1.5.0-binaries-win64-gpu-python-flir-3d_recommended\\bin\\OpenPoseDemo.exe --video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\videos\\768fa72_14-test-trials.mp4\" --write_video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\\768fa72_14-test-trialscamera1_output.avi\" --write_json \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\" --display \"1\" --hand \"\"\n",
      "Done 768fa72_14-test-trialscamera1\n",
      "It took 220 seconds for conversion.\n",
      "videos.json updated\n",
      "\n",
      "\n",
      "Staring openpose processing of b22644a_04-test-trials.camera1\n",
      "C:\\Users\\cas\\openpose-1.5.0-binaries-win64-gpu-python-flir-3d_recommended\\bin\\OpenPoseDemo.exe --video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\videos\\b22644a_04-test-trials.mp4\" --write_video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\\b22644a_04-test-trialscamera1_output.avi\" --write_json \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\" --display \"1\" --hand \"\"\n",
      "Done b22644a_04-test-trialscamera1\n",
      "It took 196 seconds for conversion.\n",
      "videos.json updated\n",
      "\n",
      "\n",
      "Staring openpose processing of b22644a_06-test-trials.camera1\n",
      "C:\\Users\\cas\\openpose-1.5.0-binaries-win64-gpu-python-flir-3d_recommended\\bin\\OpenPoseDemo.exe --video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\videos\\b22644a_06-test-trials.mp4\" --write_video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\\b22644a_06-test-trialscamera1_output.avi\" --write_json \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\" --display \"1\" --hand \"\"\n",
      "Done b22644a_06-test-trialscamera1\n",
      "It took 133 seconds for conversion.\n",
      "videos.json updated\n",
      "\n",
      "\n",
      "Staring openpose processing of b22644a_08-test-trials.camera1\n",
      "C:\\Users\\cas\\openpose-1.5.0-binaries-win64-gpu-python-flir-3d_recommended\\bin\\OpenPoseDemo.exe --video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\videos\\b22644a_08-test-trials.mp4\" --write_video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\\b22644a_08-test-trialscamera1_output.avi\" --write_json \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\" --display \"1\" --hand \"\"\n",
      "Done b22644a_08-test-trialscamera1\n",
      "It took 139 seconds for conversion.\n",
      "videos.json updated\n",
      "\n",
      "\n",
      "Staring openpose processing of b22644a_10-test-trials.camera1\n",
      "C:\\Users\\cas\\openpose-1.5.0-binaries-win64-gpu-python-flir-3d_recommended\\bin\\OpenPoseDemo.exe --video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\videos\\b22644a_10-test-trials.mp4\" --write_video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\\b22644a_10-test-trialscamera1_output.avi\" --write_json \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\" --display \"1\" --hand \"\"\n",
      "Done b22644a_10-test-trialscamera1\n",
      "It took 132 seconds for conversion.\n",
      "videos.json updated\n",
      "\n",
      "\n",
      "Staring openpose processing of b22644a_12-test-trials.camera1\n",
      "C:\\Users\\cas\\openpose-1.5.0-binaries-win64-gpu-python-flir-3d_recommended\\bin\\OpenPoseDemo.exe --video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\videos\\b22644a_12-test-trials.mp4\" --write_video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\\b22644a_12-test-trialscamera1_output.avi\" --write_json \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\" --display \"1\" --hand \"\"\n",
      "Done b22644a_12-test-trialscamera1\n",
      "It took 136 seconds for conversion.\n",
      "videos.json updated\n",
      "\n",
      "\n",
      "Staring openpose processing of b22644a_14-test-trials.camera1\n",
      "C:\\Users\\cas\\openpose-1.5.0-binaries-win64-gpu-python-flir-3d_recommended\\bin\\OpenPoseDemo.exe --video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\videos\\b22644a_14-test-trials.mp4\" --write_video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\\b22644a_14-test-trialscamera1_output.avi\" --write_json \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\" --display \"1\" --hand \"\"\n",
      "Done b22644a_14-test-trialscamera1\n",
      "It took 187 seconds for conversion.\n",
      "videos.json updated\n",
      "\n",
      "\n",
      "Staring openpose processing of CasparA_04-test-trials.camera1\n",
      "C:\\Users\\cas\\openpose-1.5.0-binaries-win64-gpu-python-flir-3d_recommended\\bin\\OpenPoseDemo.exe --video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\videos\\CasparA_04-test-trials.mp4\" --write_video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\\CasparA_04-test-trialscamera1_output.avi\" --write_json \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\" --display \"1\" --hand \"\"\n",
      "Done CasparA_04-test-trialscamera1\n",
      "It took 198 seconds for conversion.\n",
      "videos.json updated\n",
      "\n",
      "\n",
      "Staring openpose processing of CasparA_06-test-trials.camera1\n",
      "C:\\Users\\cas\\openpose-1.5.0-binaries-win64-gpu-python-flir-3d_recommended\\bin\\OpenPoseDemo.exe --video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\videos\\CasparA_06-test-trials.mp4\" --write_video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\\CasparA_06-test-trialscamera1_output.avi\" --write_json \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\" --display \"1\" --hand \"\"\n",
      "Done CasparA_06-test-trialscamera1\n",
      "It took 144 seconds for conversion.\n",
      "videos.json updated\n",
      "\n",
      "\n",
      "Staring openpose processing of CasparA_08-test-trials.camera1\n",
      "C:\\Users\\cas\\openpose-1.5.0-binaries-win64-gpu-python-flir-3d_recommended\\bin\\OpenPoseDemo.exe --video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\videos\\CasparA_08-test-trials.mp4\" --write_video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\\CasparA_08-test-trialscamera1_output.avi\" --write_json \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\" --display \"1\" --hand \"\"\n",
      "Done CasparA_08-test-trialscamera1\n",
      "It took 147 seconds for conversion.\n",
      "videos.json updated\n",
      "\n",
      "\n",
      "Staring openpose processing of CasparA_10-test-trials.camera1\n",
      "C:\\Users\\cas\\openpose-1.5.0-binaries-win64-gpu-python-flir-3d_recommended\\bin\\OpenPoseDemo.exe --video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\videos\\CasparA_10-test-trials.mp4\" --write_video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\\CasparA_10-test-trialscamera1_output.avi\" --write_json \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\" --display \"1\" --hand \"\"\n",
      "Done CasparA_10-test-trialscamera1\n",
      "It took 140 seconds for conversion.\n",
      "videos.json updated\n",
      "\n",
      "\n",
      "Staring openpose processing of CasparA_12-test-trials.camera1\n",
      "C:\\Users\\cas\\openpose-1.5.0-binaries-win64-gpu-python-flir-3d_recommended\\bin\\OpenPoseDemo.exe --video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\videos\\CasparA_12-test-trials.mp4\" --write_video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\\CasparA_12-test-trialscamera1_output.avi\" --write_json \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\" --display \"1\" --hand \"\"\n",
      "Done CasparA_12-test-trialscamera1\n",
      "It took 142 seconds for conversion.\n",
      "videos.json updated\n",
      "\n",
      "\n",
      "Staring openpose processing of CasparA_14-test-trials.camera1\n",
      "C:\\Users\\cas\\openpose-1.5.0-binaries-win64-gpu-python-flir-3d_recommended\\bin\\OpenPoseDemo.exe --video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\videos\\CasparA_14-test-trials.mp4\" --write_video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\\CasparA_14-test-trialscamera1_output.avi\" --write_json \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\" --display \"1\" --hand \"\"\n",
      "Done CasparA_14-test-trialscamera1\n",
      "It took 189 seconds for conversion.\n",
      "videos.json updated\n",
      "\n",
      "\n",
      "Staring openpose processing of SineadR_04-test-trials.camera1\n",
      "C:\\Users\\cas\\openpose-1.5.0-binaries-win64-gpu-python-flir-3d_recommended\\bin\\OpenPoseDemo.exe --video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\videos\\SineadR_04-test-trials.mp4\" --write_video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\\SineadR_04-test-trialscamera1_output.avi\" --write_json \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\" --display \"1\" --hand \"\"\n",
      "Done SineadR_04-test-trialscamera1\n",
      "It took 135 seconds for conversion.\n",
      "videos.json updated\n",
      "\n",
      "\n",
      "Staring openpose processing of SineadR_06-test-trials.camera1\n",
      "C:\\Users\\cas\\openpose-1.5.0-binaries-win64-gpu-python-flir-3d_recommended\\bin\\OpenPoseDemo.exe --video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\videos\\SineadR_06-test-trials.mp4\" --write_video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\\SineadR_06-test-trialscamera1_output.avi\" --write_json \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\" --display \"1\" --hand \"\"\n",
      "Done SineadR_06-test-trialscamera1\n",
      "It took 95 seconds for conversion.\n",
      "videos.json updated\n",
      "\n",
      "\n",
      "Staring openpose processing of SineadR_08-test-trials.camera1\n",
      "C:\\Users\\cas\\openpose-1.5.0-binaries-win64-gpu-python-flir-3d_recommended\\bin\\OpenPoseDemo.exe --video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\videos\\SineadR_08-test-trials.mp4\" --write_video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\\SineadR_08-test-trialscamera1_output.avi\" --write_json \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\" --display \"1\" --hand \"\"\n",
      "Done SineadR_08-test-trialscamera1\n",
      "It took 98 seconds for conversion.\n",
      "videos.json updated\n",
      "\n",
      "\n",
      "Staring openpose processing of SineadR_10-test-trials.camera1\n",
      "C:\\Users\\cas\\openpose-1.5.0-binaries-win64-gpu-python-flir-3d_recommended\\bin\\OpenPoseDemo.exe --video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\videos\\SineadR_10-test-trials.mp4\" --write_video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\\SineadR_10-test-trialscamera1_output.avi\" --write_json \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\" --display \"1\" --hand \"\"\n",
      "Done SineadR_10-test-trialscamera1\n",
      "It took 96 seconds for conversion.\n",
      "videos.json updated\n",
      "\n",
      "\n",
      "Staring openpose processing of SineadR_12-test-trials.camera1\n",
      "C:\\Users\\cas\\openpose-1.5.0-binaries-win64-gpu-python-flir-3d_recommended\\bin\\OpenPoseDemo.exe --video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\videos\\SineadR_12-test-trials.mp4\" --write_video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\\SineadR_12-test-trialscamera1_output.avi\" --write_json \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\" --display \"1\" --hand \"\"\n",
      "Done SineadR_12-test-trialscamera1\n",
      "It took 92 seconds for conversion.\n",
      "videos.json updated\n",
      "\n",
      "\n",
      "Staring openpose processing of SineadR_14-test-trials.camera1\n",
      "C:\\Users\\cas\\openpose-1.5.0-binaries-win64-gpu-python-flir-3d_recommended\\bin\\OpenPoseDemo.exe --video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\videos\\SineadR_14-test-trials.mp4\" --write_video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\\SineadR_14-test-trialscamera1_output.avi\" --write_json \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\DrumTutorial\\openpose\" --display \"1\" --hand \"\"\n",
      "Done SineadR_14-test-trialscamera1\n",
      "It took 95 seconds for conversion.\n",
      "videos.json updated\n"
     ]
    }
   ],
   "source": [
    "currdir =  os.getcwd() + \"\\\\\" #keep track of current directory so we can change back to it after processing\n",
    "abs_videos_out_openpose = os.path.abspath(videos_out_openpose)\n",
    "\n",
    "optstring = \"\"\n",
    "for key in params:\n",
    "    optstring += \" --\" + key +  ' \"' + params[key] + '\"' #need to quote paths\n",
    "\n",
    "print(optstring)\n",
    "\n",
    "\n",
    "count = 0\n",
    "os.chdir(openposepath)\n",
    "for vid in videos:\n",
    "    #first we need base name of video for the output file name\n",
    "    video_outname = vid + \"_output.avi\"\n",
    "    for cam in videos[vid]:\n",
    "        print(\"\\n\\nStaring openpose processing of \" + vid + \".\" + cam )\n",
    "        if videos[vid][cam][\"openpose\"][\"exitcode\"] == 0:\n",
    "            print(\"Already processed\", videos[vid][cam][\"openpose\"][\"when\"] )\n",
    "        else:\n",
    "            try:\n",
    "                # Log the time\n",
    "                time_start = time.time()\n",
    "                video = ' --video \"' + videos[vid][cam][\"fullpath\"] + '\"'\n",
    "                if createoutputvideo:\n",
    "                    video_out = ' --write_video \"' + abs_videos_out_openpose + '\\\\' + vid + cam + \"_output.avi\" + '\"'\n",
    "                else:\n",
    "                    video_out = \"\"\n",
    "                openposecommand = openposeapp + video + video_out + optstring\n",
    "                print(openposecommand)\n",
    "                exitcode = os.system(openposecommand)\n",
    "                videos[vid][cam][\"openpose\"][\"exitcode\"] = exitcode\n",
    "                videos[vid][cam][\"openpose\"][\"optstring\"] = optstring\n",
    "                videos[vid][cam][\"openpose\"][\"handdata\"] = includeHands\n",
    "                # Log the time again\n",
    "                time_end = time.time()\n",
    "                if (exitcode == 0):\n",
    "                    videos[vid][cam][\"index\"] = count  #TODO - Use this\n",
    "                    count += 1\n",
    "                    videos[vid][cam][\"openpose\"][\"exitcode\"] = 0\n",
    "                    videos[vid][cam][\"openpose\"][\"when\"] = datetime.now().isoformat()\n",
    "                    videos[vid][cam][\"openpose\"][\"out\"] = videos_out_openpose + '\\\\' + video_outname\n",
    "                    print (\"Done \" + vid + cam)\n",
    "                    print (\"It took %d seconds for conversion.\" % (time_end-time_start))\n",
    "                else:\n",
    "                    videos[vid][cam][\"openpose\"][\"exitcode\"] = exitcode\n",
    "                    videos[vid][cam][\"openpose\"][\"when\"] = datetime.now().isoformat()\n",
    "                    print(\"OpenPose error. Exit code %d\" % exitcode)\n",
    "                    print (\"Conversion failed after %d seconds.\" % (time_end-time_start))\n",
    "            except Exception as e:\n",
    "                print(\"Error: \", e)\n",
    "                os.chdir(currdir)\n",
    "                pass\n",
    "    #after each new id we save the json data\n",
    "    with open(videosjson, 'w') as outfile:\n",
    "        json.dump(videos, outfile)\n",
    "        print('videos.json updated')\n",
    "\n",
    "\n",
    "#change the directory back\n",
    "os.chdir(currdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Gather the data into useable format.\n",
    "\n",
    "OpenPose has created one JSON file per frame of video. We want to group these up into bigger arrays.\n",
    "\n",
    "This routine needs to know where to find the processed videos and what are the base names. These are listed in the `videos.json` file we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve the list of base names of processed videos.\n",
    "with open(videosjson) as json_file:\n",
    "    videos = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First find out the height, width and frames per second for each video and add this to `videos.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a note of some video properties\n",
    "\n",
    "for vid in videos:\n",
    "    for cam in videos[vid]:\n",
    "        cap = cv2.VideoCapture(videos[vid][cam][\"fullpath\"]) # 0=camera\n",
    "        if cap.isOpened():\n",
    "            videos[vid][cam][\"height\"] = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "            videos[vid][cam][\"width\"] = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "            videos[vid][cam][\"fps\"] = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "            videos[vid][cam][\"n_frames\"] = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional\n",
    "#print these out to remind ourselves.\n",
    "for vid in videos:\n",
    "    print(vid, \":\")\n",
    "    for cam in videos[vid]:\n",
    "        print(videos[vid][cam])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Extracting all the numeric data from the json files\n",
    "\n",
    "We loop through the list of names in `videos` and search for all json files associated with that name. We then extract all the coordinates and confidence scores for all identified people in each frame and store them in one big multidimensional padded array.\n",
    "\n",
    "```\n",
    "1st dimension - number of videos\n",
    "2nd dimension - max number of cameras (often 1 but sometimes 2 or 3)\n",
    "3rd dimension - max nummber of frames\n",
    "4th dimension - max number of people\n",
    "5th dimension - number of values (per person) output by openpose\n",
    "```\n",
    "\n",
    "For example, if we had the following videos\n",
    "\n",
    "```\n",
    "video1 - cam1 - 200 frames  - 3 people (max)\n",
    "video1 - cam2 - 200 frames  - 3 people (max)\n",
    "video2 - 203 frames  - 2 people (max)\n",
    "video3 - 219 frames  - 4 people (max)\n",
    "```\n",
    "\n",
    "then we'd create a `3 x 2 x 219 x 4 x 75` array.\n",
    "\n",
    "First we see how big the dimensions of the array have to be.\n",
    "And create an numpy array called `keypoints_array` big enough to hold all of this.\n",
    "\n",
    "As a sanity check we count the number of frames processed by openpose. Ought to be same as above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video 1fa339b_04-test-trials camera1 has 922 frames.\n",
      "Video 1fa339b_06-test-trials camera1 has 616 frames.\n",
      "Video 1fa339b_08-test-trials camera1 has 630 frames.\n",
      "Video 1fa339b_10-test-trials camera1 has 664 frames.\n",
      "Video 1fa339b_12-test-trials camera1 has 661 frames.\n",
      "Video 1fa339b_14-test-trials camera1 has 917 frames.\n",
      "Video 768fa72_04-test-trials camera1 has 930 frames.\n",
      "Video 768fa72_06-test-trials camera1 has 664 frames.\n",
      "Video 768fa72_08-test-trials camera1 has 659 frames.\n",
      "Video 768fa72_10-test-trials camera1 has 640 frames.\n",
      "Video 768fa72_12-test-trials camera1 has 625 frames.\n",
      "Video 768fa72_14-test-trials camera1 has 919 frames.\n",
      "Video b22644a_04-test-trials camera1 has 934 frames.\n",
      "Video b22644a_06-test-trials camera1 has 634 frames.\n",
      "Video b22644a_08-test-trials camera1 has 673 frames.\n",
      "Video b22644a_10-test-trials camera1 has 657 frames.\n",
      "Video b22644a_12-test-trials camera1 has 658 frames.\n",
      "Video b22644a_14-test-trials camera1 has 923 frames.\n",
      "Video CasparA_04-test-trials camera1 has 942 frames.\n",
      "Video CasparA_06-test-trials camera1 has 688 frames.\n",
      "Video CasparA_08-test-trials camera1 has 700 frames.\n",
      "Video CasparA_10-test-trials camera1 has 669 frames.\n",
      "Video CasparA_12-test-trials camera1 has 676 frames.\n",
      "Video CasparA_14-test-trials camera1 has 928 frames.\n",
      "Video SineadR_04-test-trials camera1 has 956 frames.\n",
      "Video SineadR_06-test-trials camera1 has 682 frames.\n",
      "Video SineadR_08-test-trials camera1 has 678 frames.\n",
      "Video SineadR_10-test-trials camera1 has 679 frames.\n",
      "Video SineadR_12-test-trials camera1 has 663 frames.\n",
      "Video SineadR_14-test-trials camera1 has 648 frames.\n",
      "Initialise numpy array of size (30, 3, 956, 15, 75)\n"
     ]
    }
   ],
   "source": [
    "nvideos = len(videos)\n",
    "maxcameras = 1\n",
    "#maxcameras = 3\n",
    "maxframes = 0\n",
    "maxpeople = 10 #maximum people we might expect (large upper bound)\n",
    "ncoords = 75 #the length of the array coming back from openpose x,y coords of each point plus cafs\n",
    "hcoords = 63\n",
    "\n",
    "for vid in videos:\n",
    "    for cam in videos[vid]:\n",
    "        #use glob to get all the individual json files.\n",
    "        videoname = videos[vid][cam][\"stemname\"]\n",
    "        alljson = glob.glob(videos_out_openpose + \"\\\\\" + videoname + \"_*.json\")\n",
    "        nframes = len(alljson)\n",
    "        print(\"Video\", vid, cam, \"has {0} frames.\".format(nframes))\n",
    "        videos[vid][cam][\"frames\"] = nframes\n",
    "        maxframes = max(maxframes,nframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxpeople = 10 #maximum people we might expect (large upper bound)\n",
    "\n",
    "keypoints_array = np.zeros([nvideos,maxcameras, maxframes,maxpeople,ncoords]) #big array to hold all the numbers\n",
    "\n",
    "#also create arrays for hand data\n",
    "righthand_array = np.zeros([nvideos,maxcameras, maxframes,maxpeople,hcoords]) #big array to hold all the numbers\n",
    "lefthand_array  = np.zeros([nvideos,maxcameras, maxframes,maxpeople,hcoords]) #big array to hold all the numbers\n",
    "\n",
    "print(\"Initialise numpy array of size\", keypoints_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now loop through all the videos copying the frame data into our big `keypoints_array` and also seeing how many people (max) are detected in each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#update the json file in the video out directory\n",
    "with open(videosjson, 'w') as outfile:\n",
    "    json.dump(videos, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#update the json file in the video out directory\n",
    "with open(videosjson, 'w') as outfile:\n",
    "    json.dump(videos, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video 1fa339b_04-test-trials camera1 has 3 people detected.\n",
      "Video 1fa339b_06-test-trials camera1 has 3 people detected.\n",
      "Video 1fa339b_08-test-trials camera1 has 4 people detected.\n",
      "Video 1fa339b_10-test-trials camera1 has 3 people detected.\n",
      "Video 1fa339b_12-test-trials camera1 has 4 people detected.\n",
      "Video 1fa339b_14-test-trials camera1 has 4 people detected.\n",
      "Video 768fa72_04-test-trials camera1 has 3 people detected.\n",
      "Video 768fa72_06-test-trials camera1 has 4 people detected.\n",
      "Video 768fa72_08-test-trials camera1 has 4 people detected.\n",
      "Video 768fa72_10-test-trials camera1 has 3 people detected.\n",
      "Video 768fa72_12-test-trials camera1 has 4 people detected.\n",
      "Video 768fa72_14-test-trials camera1 has 3 people detected.\n",
      "Video b22644a_04-test-trials camera1 has 3 people detected.\n",
      "Video b22644a_06-test-trials camera1 has 3 people detected.\n",
      "Video b22644a_08-test-trials camera1 has 3 people detected.\n",
      "Video b22644a_10-test-trials camera1 has 3 people detected.\n",
      "Video b22644a_12-test-trials camera1 has 3 people detected.\n",
      "Video b22644a_14-test-trials camera1 has 3 people detected.\n",
      "Video CasparA_04-test-trials camera1 has 1 people detected.\n",
      "Video CasparA_06-test-trials camera1 has 1 people detected.\n",
      "Video CasparA_08-test-trials camera1 has 1 people detected.\n",
      "Video CasparA_10-test-trials camera1 has 1 people detected.\n",
      "Video CasparA_12-test-trials camera1 has 1 people detected.\n",
      "Video CasparA_14-test-trials camera1 has 1 people detected.\n",
      "Video SineadR_04-test-trials camera1 has 3 people detected.\n",
      "Video SineadR_06-test-trials camera1 has 3 people detected.\n",
      "Video SineadR_08-test-trials camera1 has 3 people detected.\n",
      "Video SineadR_10-test-trials camera1 has 3 people detected.\n",
      "Video SineadR_12-test-trials camera1 has 3 people detected.\n",
      "Video SineadR_14-test-trials camera1 has 3 people detected.\n",
      "keypoints_array has size (30, 3, 956, 4, 75)\n"
     ]
    }
   ],
   "source": [
    "npeople = np.zeros(maxframes)  #an array to track how many people detected per frame.\n",
    "globalmaxpeople =  0\n",
    "v = -1\n",
    "\n",
    "for vid in videos:\n",
    "    v += 1  #index for this subject\n",
    "    c = -1\n",
    "    for cam in videos[vid]:\n",
    "        c += 1  #index for this camera\n",
    "        #use glob to get all the individual json files.\n",
    "        alljson = glob.glob(videos_out_openpose + \"\\\\\" + videos[vid][cam][\"stemname\"] + \"_*.json\")\n",
    "        f = 0\n",
    "        for frame in alljson:\n",
    "            with open(frame, \"r\") as read_file:\n",
    "                data = json.load(read_file)\n",
    "                pers = 0\n",
    "                for p in data[\"people\"]:\n",
    "                    keypoints_array[v,c,i,j,:]= p[\"pose_keypoints_2d\"]\n",
    "                    if includeHands:\n",
    "                        righthand_array[v,c,i,j,:]= p[\"hand_right_keypoints_2d\"]\n",
    "                        lefthand_array[v,c,i,j,:] = p[\"hand_left_keypoints_2d\"]\n",
    "                    j += 1\n",
    "                npeople[i] = j\n",
    "                i += 1\n",
    "        #end loop for this video\n",
    "        people = int(max(npeople))\n",
    "        print(\"Video\", vid, cam, \"has {0} people detected.\".format(people))\n",
    "        videos[vid][cam][\"maxpeople\"] = people\n",
    "        videos[vid][cam][\"v\"] = v  #might be useful to have these indices available\n",
    "        videos[vid][cam][\"c\"] = c\n",
    "        #how many people did it contain? Is this biggest number so far?\n",
    "        globalmaxpeople = max(globalmaxpeople, people)\n",
    "\n",
    "\n",
    "#and just like that n videos have been reduced to a big block of people coords.\n",
    "#we now truncate the array for the maximum number of people as the rest of it is all zeros\n",
    "\n",
    "keypoints_array = np.delete(keypoints_array,np.s_[int(globalmaxpeople):],3)\n",
    "\n",
    "if includeHands:\n",
    "    righthand_array = np.delete(righthand_array,np.s_[int(globalmaxpeople):],3)\n",
    "    lefthand_array  = np.delete(lefthand_array, np.s_[int(globalmaxpeople):],3)\n",
    "\n",
    "print(\"keypoints_array has size\", keypoints_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Save the data!\n",
    "\n",
    "Saving the data at this stage so we don't have to repeat these steps again if we reorganise or reanalyse the data.\n",
    "\n",
    "We create a compressed NumPy array `allframedata.npz` containing the person location data for all the videos.\n",
    "\n",
    "We also update the `videos.json` file with more info about the videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "#update the json file in the video out directory\n",
    "with open(videosjson, 'w') as outfile:\n",
    "    json.dump(videos, outfile)\n",
    "\n",
    "# in the time series folder we save the data file.\n",
    "#in a compressed format as it has a lot of empty values\n",
    "np.savez_compressed(videos_out_timeseries + \"\\\\\" + settings[\"filenames\"][\"alldatanpz\"], keypoints_array=keypoints_array)\n",
    "\n",
    "if includeHands:\n",
    "    np.savez_compressed(videos_out_timeseries + '\\\\' + settings[\"filenames\"][\"righthandnpz\"], keypoints_array=righthand_array)\n",
    "    np.savez_compressed(videos_out_timeseries + '\\\\' + settings[\"filenames\"][\"lefthandnpz\"],  keypoints_array=lefthand_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ee4b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#after each new id we save the json data\n",
    "settings[\"lastUpdate\"] = datetime.now().isoformat()\n",
    "with open(settingsjson, 'w') as outfile:\n",
    "    json.dump(settings, outfile)\n",
    "    print('settings.json updated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### That's it.\n",
    "\n",
    "Now go onto [Step 2 - Organising the data](Step2.OrganiseData.ipynb)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
