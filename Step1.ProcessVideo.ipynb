{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Actor Synchroncy and Causality (VASC)\n",
    "## RAEng: Measuring Responsive Caregiving Project\n",
    "### Caspar Addyman, 2020\n",
    "### https://github.com/infantlab/VASC\n",
    "\n",
    "# Step 1  Process videos using OpenPose\n",
    "\n",
    "This script uses [OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) human figure recognition neural network to create labeled wireframes for each figure in each frame of a video. It uses the [OpenPoseDemo](https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/demo_overview.md) executable which must already be downloaded and installed.\n",
    "\n",
    "Additionally, you need to download the trained neural-network models that OpenPose uses. To do this go to the `models` subdirectory of OpenPose directory, and double-click / run the `models.bat` script.\n",
    "\n",
    "The `openposedemo` bin/exe file can be run manually from the command line.  \n",
    "\n",
    "OpenPoseDemo will go through a video frame by frame outputing a JSON file for each frame that contain\n",
    "s a set of coordinate points and for a wireframe for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what python libraries \n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin\\OpenPoseDemo.exe\n"
     ]
    }
   ],
   "source": [
    "jupwd =  os.getcwd() + \"\\\\\"\n",
    "# locations of videos and output\n",
    "videos_in = wd + \"..\\\\videos_in\\\\\" \n",
    "videos_out_openpose = wd + \"..\\\\videos_out_openpose\"\n",
    "\n",
    "# location of openposedemo - THIS WILL BE DIFFERENT ON YOUR COMPUTER\n",
    "openposepath = \"C:\\\\Users\\\\cas\\\\openpose-1.5.0-binaries-win64-gpu-python-flir-3d_recommended\\\\\"\n",
    "\n",
    "if sys.platform == \"win32\":\n",
    "    app = \"bin\\\\OpenPoseDemo.exe\"\n",
    "else:\n",
    "    app = 'bin\\\\OpenPoseDemo.bin'\n",
    "\n",
    "fullopenposeapp = openposepath + app\n",
    "shortopenposeapp = app\n",
    "print(shortopenposeapp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\cas\\\\OneDrive - Goldsmiths College\\\\Projects\\\\Measuring Responsive Caregiving\\\\VASC\\\\..\\\\videos_in\\\\secure.avi']\n",
      "['C:\\\\Users\\\\cas\\\\OneDrive - Goldsmiths College\\\\Projects\\\\Measuring Responsive Caregiving\\\\VASC\\\\..\\\\videos_in\\\\stillface.mp4']\n"
     ]
    }
   ],
   "source": [
    "#first get list of videos in the inbox\n",
    "avis = glob.glob(videos_in + \"*.avi\")\n",
    "mp4s = glob.glob(videos_in + \"*.mp4\")\n",
    "\n",
    "print(avis)\n",
    "print(mp4s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\cas\\\\OneDrive - Goldsmiths College\\\\Projects\\\\Measuring Responsive Caregiving\\\\VASC\\\\..\\\\videos_in\\\\secure.avi', 'C:\\\\Users\\\\cas\\\\OneDrive - Goldsmiths College\\\\Projects\\\\Measuring Responsive Caregiving\\\\VASC\\\\..\\\\videos_in\\\\stillface.mp4']\n"
     ]
    }
   ],
   "source": [
    "#For the moment we will manually specify what videos to process. \n",
    "#TODO generate a list of force or skip videos to automate things slightly\n",
    "allvideos = avis\n",
    "allvideos.extend(mp4s)\n",
    "print(allvideos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\cas\\\\OneDrive - Goldsmiths College\\\\Projects\\\\Measuring Responsive Caregiving\\\\VASC\\\\..\\\\videos_in\\\\secure.avi', 'C:\\\\Users\\\\cas\\\\OneDrive - Goldsmiths College\\\\Projects\\\\Measuring Responsive Caregiving\\\\VASC\\\\..\\\\videos_in\\\\stillface.mp4']\n"
     ]
    }
   ],
   "source": [
    "print(avis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To operate OpenPose we pass a set of parameters to the demo executable. For the full list of options see  [OpenPoseDemo](https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/demo_overview.md)\n",
    "\n",
    "Our main parameters are\n",
    "\n",
    "```\n",
    "--video        path\\to\\video_to_process   #input video\n",
    "--write_json   path\\to\\output_directory   #one json file per frame\n",
    "--write_video  path\\to\\output_directory   #video with identified figures\n",
    "--write_images path\\to\\output_directory   #one image per frame with wireframes\n",
    "--disable_blending true/false             # wireframes on black background (true) or blended on top of video (false)\n",
    " ```\n",
    "\n",
    "Other useful params\n",
    " ```\n",
    "--frame_first  100    #start from frame 100\n",
    "--display 0           #don't show the images as they are processed\n",
    " ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict()\n",
    "params[\"write_json\"] = videos_out_openpose\n",
    "params[\"write_images\"] = videos_out_openpose  #for the moment dump images in output file - TODO name subfolder\n",
    "params[\"disable_blending\"] = \"true\"\n",
    "params[\"display\"]  = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secure\n"
     ]
    }
   ],
   "source": [
    "vid = allvideos[0]\n",
    "\n",
    "base = os.path.basename(vid)\n",
    "vidbasename = os.path.splitext(base)\n",
    "print(vidbasename[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --write_json \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\..\\videos_out_openpose\" --write_images \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\..\\videos_out_openpose\" --disable_blending \"true\" --display \"1\"\n",
      "\n",
      "\n",
      "Staring openpose processing of C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\..\\videos_in\\secure.avi\n",
      "bin\\OpenPoseDemo.exe --video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\..\\videos_in\\secure.avi\" --write_video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\..\\videos_out_openpose\\secure_output.avi\" --write_json \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\..\\videos_out_openpose\" --write_images \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\..\\videos_out_openpose\" --disable_blending \"true\" --display \"1\"\n",
      "OpenPose error. Exit code -1\n",
      "\n",
      "\n",
      "Staring openpose processing of C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\..\\videos_in\\stillface.mp4\n",
      "bin\\OpenPoseDemo.exe --video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\..\\videos_in\\stillface.mp4\" --write_video \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\..\\videos_out_openpose\\stillface_output.avi\" --write_json \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\..\\videos_out_openpose\" --write_images \"C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\..\\videos_out_openpose\" --disable_blending \"true\" --display \"1\"\n",
      "Done C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\..\\videos_in\\stillface.mp4\n",
      "It took 388 seconds for conversion.\n"
     ]
    }
   ],
   "source": [
    "optstring = \"\"\n",
    "for key in params:\n",
    "    optstring += \" --\" + key +  ' \"' + params[key] + '\"' #need to quote paths \n",
    "\n",
    "print(optstring)\n",
    "\n",
    "os.chdir(openposepath)\n",
    "for vid in allvideos:\n",
    "    #first we need base name of video for the output file name\n",
    "    base = os.path.basename(vid)\n",
    "    vidbasename = os.path.splitext(base)\n",
    "    video_outname = vidbasename[0] + \"_output.avi\"\n",
    "    try:\n",
    "        print(\"\\n\\nStaring openpose processing of \" + vid)\n",
    "        # Log the time\n",
    "        time_start = time.time()\n",
    "        video = ' --video \"' + vid + '\"'\n",
    "        video_out = ' --write_video \"' + videos_out_openpose + '\\\\' + video_outname + '\"'\n",
    "        opbin = shortopenposeapp + video + video_out + optstring\n",
    "        print(opbin)\n",
    "        exitcode = os.system(opbin)\n",
    "        # Log the time again\n",
    "        time_end = time.time()\n",
    "        if (exitcode == 0):\n",
    "            # Print stats\n",
    "            print (\"Done \" + vid)\n",
    "            print (\"It took %d seconds for conversion.\" % (time_end-time_start))\n",
    "        else:\n",
    "            print(\"OpenPose error. Exit code %d\" % exitcode)\n",
    "    except Exception as e:\n",
    "        print(\"Error: \", e)\n",
    "        pass\n",
    "    \n",
    "os.chdir(jupwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(allvideos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def video_to_frames(input_loc, output_loc):\n",
    "    \"\"\"Function to extract frames from input video file\n",
    "    and save them as separate frames in an output directory.\n",
    "    Args:\n",
    "        input_loc: Input video file.\n",
    "        output_loc: Output directory to save the frames.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.mkdir(output_loc)\n",
    "    except OSError:\n",
    "        pass\n",
    "    # Log the time\n",
    "    time_start = time.time()\n",
    "    # Start capturing the feed\n",
    "    cap = cv2.VideoCapture(input_loc)\n",
    "    # Find the number of frames\n",
    "    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - 1\n",
    "    print (\"Number of frames: \", video_length)\n",
    "    count = 0\n",
    "    print (\"Converting video..\\n\")\n",
    "    # Start converting the video\n",
    "    while cap.isOpened():\n",
    "        # Extract the frame\n",
    "        ret, frame = cap.read()\n",
    "        # Write the results back to output location.\n",
    "        cv2.imwrite(output_loc + \"/%#05d.jpg\" % (count+1), frame)\n",
    "        count = count + 1\n",
    "        # If there are no more frames left\n",
    "        if (count > (video_length-1)):\n",
    "            # Log the time again\n",
    "            time_end = time.time()\n",
    "            # Release the feed\n",
    "            cap.release()\n",
    "            # Print stats\n",
    "            print (\"Done extracting frames.\\n%d frames extracted\" % count)\n",
    "            print (\"It took %d seconds for conversion.\" % (time_end-time_start))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
