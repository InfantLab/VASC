{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Actor Synchroncy and Causality (VASC)\n",
    "## RAEng: Measuring Responsive Caregiving Project\n",
    "### Caspar Addyman, 2020\n",
    "### https://github.com/infantlab/VASC\n",
    "\n",
    "# Step 2: Reorganise the OpenPose JSON wire frame data\n",
    "\n",
    "This script uses output from [OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) human figure recognition neural network to create labeled wireframes for each figure in each frame of a video. It uses the Python API version of OpenPose.\n",
    "\n",
    "Points are labelled as so:\n",
    "```\n",
    "COCO Output Format\n",
    "Nose – 0, Neck – 1, Right Shoulder – 2, Right Elbow – 3, Right Wrist – 4,\n",
    "Left Shoulder – 5, Left Elbow – 6, Left Wrist – 7, Right Hip – 8,\n",
    "Right Knee – 9, Right Ankle – 10, Left Hip – 11, Left Knee – 12,\n",
    "LAnkle – 13, Right Eye – 14, Left Eye – 15, Right Ear – 16,\n",
    "Left Ear – 17, Background – 18\n",
    "```\n",
    "\n",
    "The `write_json flag` saves the people pose data using a custom JSON writer. Each JSON file has a people array of objects, where each object has:\n",
    "\n",
    "> An array pose_keypoints_2d containing the body part locations and detection confidence formatted as x1,y1,c1,x2,y2,c2,.... The coordinates x and y can be normalized to the range [0,1], [-1,1], [0, source size], [0, output size], etc., depending on the flag keypoint_scale (see flag for more information), while c is the confidence score in the range [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import glob\n",
    "import json\n",
    "import cv2  #computervision toolkit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nPoints = 18\n",
    "\n",
    "# COCO Output Format\n",
    "keypointsMapping = ['Nose', 'Neck', 'R-Sho', 'R-Elb', 'R-Wr', 'L-Sho', \n",
    "                    'L-Elb', 'L-Wr', 'R-Hip', 'R-Knee', 'R-Ank', 'L-Hip', \n",
    "                    'L-Knee', 'L-Ank', 'R-Eye', 'L-Eye', 'R-Ear', 'L-Ear']\n",
    "\n",
    "POSE_PAIRS = [[1,2], [1,5], [2,3], [3,4], [5,6], [6,7],\n",
    "              [1,8], [8,9], [9,10], [1,11], [11,12], [12,13],\n",
    "              [1,0], [0,14], [14,16], [0,15], [15,17],\n",
    "              [2,17], [5,16] ]\n",
    "\n",
    "# index of pafs correspoding to the POSE_PAIRS\n",
    "# e.g for POSE_PAIR(1,2), the PAFs are located at indices (31,32) of output, Similarly, (1,5) -> (39,40) and so on.\n",
    "mapIdx = [[31,32], [39,40], [33,34], [35,36], [41,42], [43,44],\n",
    "          [19,20], [21,22], [23,24], [25,26], [27,28], [29,30],\n",
    "          [47,48], [49,50], [53,54], [51,52], [55,56],\n",
    "          [37,38], [45,46]]\n",
    "\n",
    "pointcolors = [ [0,100,255], [0,100,255], [0,255,255], [0,100,255], [0,255,255], [0,100,255],\n",
    "         [0,255,0], [255,200,100], [255,0,255], [0,255,0], [255,200,100], [255,0,255],\n",
    "         [0,0,255], [255,0,0], [200,200,0], [255,0,0], [200,200,0], [0,0,0]]\n",
    "\n",
    "#will colour each person 0:9 a different colour to help us keep track\n",
    "personcolours = [ [255,0,0], [0,255,0], [0,0,255],[0,255,255], [255,0,255], [255,255,0],[128,255,255], [255,128,255], [255,255,128],[0,0,0]]\n",
    "\n",
    "#useful to have the indices of the x & y coords and the confidence scores \n",
    "#recall that we get them in the order [x0,y0,c0,x1,y1,c1,x2,etc]\n",
    "xs = [0+3*i for i in range(nPoints)]\n",
    "ys = [1+3*i for i in range(nPoints)]\n",
    "cs = [2+3*i for i in range(nPoints)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Where are the JSON files?\n",
    "\n",
    "This routine needs to know where to find the processed videos and what are the base names. These are listed in the `processed.json` file we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\..\\lookit\\\n",
      "C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\..\\lookit\\\\out\\openpose\n",
      "C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\..\\lookit\\\\out\\timeseries\n",
      "C:\\Users\\cas\\OneDrive - Goldsmiths College\\Projects\\Measuring Responsive Caregiving\\VASC\\..\\lookit\\\\out\\analyses\n"
     ]
    }
   ],
   "source": [
    "# where's the project folder?\n",
    "\n",
    "jupwd =  os.getcwd() + \"\\\\\"\n",
    "projectpath = os.getcwd() + \"\\\\..\\\\lookit\\\\\"\n",
    "\n",
    "# locations of videos and output\n",
    "videos_in = projectpath \n",
    "videos_out_openpose   = projectpath + \"\\\\out\\\\openpose\"\n",
    "videos_out_timeseries = projectpath + \"\\\\out\\\\timeseries\"\n",
    "videos_out_analyses   = projectpath + \"\\\\out\\\\analyses\"\n",
    "\n",
    "print(videos_in)\n",
    "print(videos_out_openpose)\n",
    "print(videos_out_timeseries)\n",
    "print(videos_out_analyses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lookit.01', 'lookit.02', 'lookit.03', 'lookit.04', 'lookit.05', 'lookit.06', 'lookit.07', 'lookit.08', 'lookit.09', 'lookit.10']\n"
     ]
    }
   ],
   "source": [
    "#retrieve the list of base names of processed videos.\n",
    "with open(videos_out_openpose + '\\\\processed.json') as json_file:\n",
    "    processed = json.load(json_file)\n",
    "\n",
    "print(processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract all the numeric data from the json files\n",
    "\n",
    "We loop through the list of names in `processed` and search for all json files associated with that name. We then extract all the coordinates and confidence scores for all identified people in each frame and store them in one big multidimensional padded array.\n",
    "\n",
    "```\n",
    "1st dimension - number of videos\n",
    "2nd dimension - max nummber of frames\n",
    "3rd dimension - max number of people\n",
    "4th dimension - number of values (per person) output by openpose\n",
    "```\n",
    "\n",
    "For example, if we had the following videos \n",
    "\n",
    "```\n",
    "video1 - 200 frames  - 3 people (max) \n",
    "video2 - 203 frames  - 2 people (max) \n",
    "video3 - 219 frames  - 4 people (max) \n",
    "```\n",
    "\n",
    "then we'd create a `3 x 219 x 4 x 75` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vidbasename in processed:\n",
    "    #use glob to get all the individual json files.\n",
    "    alljson = glob.glob(videos_out_openpose + \"\\\\\" + processed[0] + \"*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use glob to get all the individual json files.\n",
    "alljson = glob.glob(videos_out_openpose + \"\\\\\" + processed[0] + \"*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\cas\\\\OneDrive - Goldsmiths College\\\\Projects\\\\Measuring Responsive Caregiving\\\\VASC\\\\..\\\\lookit\\\\\\\\out\\\\openpose\\\\lookit.01*.json'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos_out_openpose + \"\\\\\" + processed[0] + \"*.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This video has 162 frames.\n"
     ]
    }
   ],
   "source": [
    "nframes = len(alljson) #how many frames in the video?\n",
    "maxpeople = 10 #maximum people we might expect (large upper bound)\n",
    "ncoords = 75 #the length of the array coming back from openpose x,y coords of each point plus pafs\n",
    "\n",
    "keypoints_list = np.zeros([nframes,maxpeople,ncoords]) #big array to hold all the numbers\n",
    "npeople = np.zeros(nframes)  #how many people detected per frame?\n",
    "print(\"This video has {0} frames.\".format(nframes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will first combine all the data into one giant numpy array. Then we will use this for everything that comes after. \n",
    "# such as normalising.. confirming that person 1 is same individal all way through etc.\n",
    "\n",
    "i = 0\n",
    "for frame in alljson:\n",
    "    with open(frame, \"r\") as read_file:\n",
    "        data = json.load(read_file)\n",
    "        j = 0\n",
    "        for p in data[\"people\"]:\n",
    "            keypoints = p[\"pose_keypoints_2d\"]  \n",
    "            keypoints_list[i,j,:]=keypoints\n",
    "            j += 1\n",
    "        npeople[i] = j\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "print(max(npeople))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's pic a frame a draw the wireframe skeleton\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thisframe = 1400\n",
    "thisperson = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"attachment_avoidant_AGRT6VjnTm8_360p.mp4\")\n",
    "\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - 1\n",
    "\n",
    "print(total_frames)\n",
    "\n",
    "framenumber = thisframe\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES,framenumber) # Where frame_no is the frame you want\n",
    "ret, frame = cap.read() # Read the frame\n",
    "#cv2.imwrite(\"frame%#05d.jpg\" % (framenumber+1), frame)\n",
    "\n",
    "plt.figure(figsize=[14,10])\n",
    "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(npeople[thisframe])\n",
    "print(keypoints_list[thisframe,thisperson,:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getkeypointcoord(keypointlist,index):\n",
    "    x = index * 3\n",
    "    #some shuffling around to get coords in right format for plotting.\n",
    "    coords = keypointlist[x:x+2]\n",
    "    coords = map(int,coords)\n",
    "    coords = tuple(coords)\n",
    "    return coords\n",
    "\n",
    "def averagePoint(keypointList,indices):\n",
    "    \"\"\"Function to find the \"centre of mass\" for this person.\n",
    "    It will take the average of the non-zero keypoints\n",
    "    Args:\n",
    "        keypointList: 1d array of keypoints.\n",
    "        indices: a set of indices to average over.\n",
    "    Returns:\n",
    "        Average\n",
    "    \"\"\"\n",
    "    tot = 0\n",
    "    N = 0\n",
    "    for i in indices:\n",
    "        if keypoints[i]>0:\n",
    "            tot += keypoints[i]\n",
    "            N += 1\n",
    "    return tot / N\n",
    "\n",
    "def diffKeypoints(keypoints1,keypoints2,indices):\n",
    "    \"\"\"Function to find how far apart one set of points is from another.\n",
    "    This is useful for seeing if we have same person labelled correctly\n",
    "    from one frame to next. If any point goes out of frame (loc == 0)\n",
    "    then we don't include that pair. \n",
    "    Args:\n",
    "        keypoints1: 1st array of keypoints.\n",
    "        keypoints2: 1st array of keypoints.\n",
    "        indices: a set of indices to compare over.\n",
    "    Returns:\n",
    "        diff per index (if any i)\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for i in indices:\n",
    "        if keypoints1[i]>0 and keypoints2[i]:\n",
    "            out.append(keypoints1[i] - keypoints2[i])\n",
    "        else:\n",
    "            out.append(None)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frameClone = frame.copy()\n",
    "for pers in range(int(npeople[thisframe])):\n",
    "    keypoints = keypoints_list[thisframe,pers,:]\n",
    "    for i in range(nPoints):\n",
    "        coords = getkeypointcoord(keypoints,i)\n",
    "        if sum(coords) > 0:\n",
    "            cv2.circle(frameClone,coords, 2, [0,0,255], -1, cv2.LINE_AA)\n",
    "\n",
    "    for i in range(nPoints):\n",
    "        l = POSE_PAIRS[i]\n",
    "        A = getkeypointcoord(keypoints,l[0])\n",
    "        B = getkeypointcoord(keypoints,l[1])\n",
    "        if sum(A) > 0 and sum(B) > 0:\n",
    "            cv2.line(frameClone, (A[0], A[1]), (B[0], B[1]), pointcolors[i], 2, cv2.LINE_AA)    \n",
    "     \n",
    "    avx = averagePoint(keypoints,xs)\n",
    "    avy = averagePoint(keypoints,ys)\n",
    "    print(avx)\n",
    "    print(avy)\n",
    "    print(averagePoint(keypoints,cs))\n",
    "\n",
    "    cgloc  = tuple((int(avx),int(avy)))\n",
    "    txtloc = tuple((int(avx),int(avy) - 30))\n",
    "    cv2.circle(frameClone,cgloc, 2, [0,0,0], -1, cv2.LINE_AA)\n",
    "    cv2.putText(frameClone, str(pers), txtloc, font, fontScale, personcolours[pers])\n",
    "\n",
    "plt.figure(figsize=[15,15])\n",
    "plt.imshow(frameClone[:,:,[2,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf() \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "plt.figure(figsize=[15,15])\n",
    "plt.imshow(frameClone[:,:,[2,1,0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npeople[thisframe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averagePoint(keypointList,indices):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# font \n",
    "font = cv2.FONT_HERSHEY_SIMPLEX   \n",
    "# fontScale \n",
    "fontScale = .7\n",
    "# Blue color in BGR \n",
    "color = (255, 0, 0) \n",
    "\n",
    "\n",
    "plt.figure(figsize=[15,15])\n",
    "plt.imshow(frameClone[:,:,[2,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"attachment_avoidant_AGRT6VjnTm8_360p_000000000181_keypoints.json\", \"r\") as read_file:\n",
    "    data = json.load(read_file)\n",
    "    \n",
    "\n",
    "detected_keypoints = []\n",
    "keypoints_list = np.zeros((1,18*3))\n",
    "keypoint_id = 0\n",
    "threshold = 0.1\n",
    "\n",
    "for p in data[\"people\"]:\n",
    "    keypoints = p[\"pose_keypoints_2d\"]        \n",
    "    keypoint_id = 0\n",
    "    keypoints_with_id = []\n",
    "    keypoints_list = keypoints\n",
    "    for part in range(nPoints):\n",
    "        thisone =  {keypointsMapping[part] : {\"x\" : keypoints[keypoint_id], \"y\" : keypoints[keypoint_id + 1], \"conf\" : keypoints[keypoint_id + 2]}}\n",
    "        keypoints_with_id.append(thisone)\n",
    "        keypoint_id += 3\n",
    "    #print(keypoints_with_id)\n",
    "    print(keypoints_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"attachment_avoidant_AGRT6VjnTm8_360p.mp4\")\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cap = cv2.VideoCapture(\"attachment_avoidant_AGRT6VjnTm8_360p.mp4\")\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - 1\n",
    "\n",
    "print(total_frames)\n",
    "\n",
    "framenumber = 1000\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES,framenumber) # Where frame_no is the frame you want\n",
    "ret, frame = cap.read() # Read the frame\n",
    "#cv2.imwrite(\"frame%#05d.jpg\" % (framenumber+1), frame)\n",
    "\n",
    "cv2.imshow('babies', frame)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2.imshow('babies', frame)\n",
    "\n",
    "for part in range(nPoints):\n",
    "    probMap = output[0,part,:,:]\n",
    "    probMap = cv2.resize(probMap, (frame.shape[1], frame.shape[0]))\n",
    "    keypoints = getKeypoints(probMap, threshold)\n",
    "    print(\"Keypoints - {} : {}\".format(keypointsMapping[part], keypoints))\n",
    "    keypoints_with_id = []\n",
    "    for i in range(len(keypoints)):\n",
    "        keypoints_with_id.append(keypoints[i] + (keypoint_id,))\n",
    "        keypoints_list = np.vstack([keypoints_list, keypoints[i]])\n",
    "        keypoint_id += 1\n",
    "\n",
    "    detected_keypoints.append(keypoints_with_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_to_frames(\"attachment_avoidant_AGRT6VjnTm8_360p.mp4\", \"./frames/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
