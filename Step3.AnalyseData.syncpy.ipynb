{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Actor Synchroncy and Causality (VASC)\n",
    "## RAEng: Measuring Responsive Caregiving Project\n",
    "### Caspar Addyman, 2020\n",
    "### https://github.com/infantlab/VASC\n",
    "\n",
    "# Step 3: Analyse the data using SyncPy\n",
    "\n",
    "This script uses output from  human figure recognition neural network to create labeled wireframes for each figure in each frame of a video. \n",
    "In this step we start with a clean numpy array of all [OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) data from all pairs of individuals. We then use [SyncPy](https://github.com/syncpy/SyncPy) for data analysis. SyncPy was developed by Giovanna Varni, Mohamed Chetouani and colleagues at the Institut des Systèmes Intelligentes et Robotique (ISIR) at the Université Pierre et Marie Curie (UPMC), Paris 6, France.  \n",
    "\n",
    "A technical paper is found here: [Varni et al. (2015)](https://dl.acm.org/doi/10.1145/2823513.2823520)\n",
    " \n",
    "\n",
    "**NOTE:**\n",
    "At present (June 2020) the official version of SyncPy is written in Python2 format. Therefore, we have forked it and created a Python3 compatible version at https://github.com/InfantLab/SyncPy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np       \n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import logging\n",
    "import ipywidgets as widgets  #let's us add buttons and sliders to this page.\n",
    "from ipycanvas import Canvas\n",
    "\n",
    "import vasc #a module of our own functions (found in vasc.py in this folder)\n",
    "\n",
    "#turn on debugging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "%pdb on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jupwd =  os.getcwd() + \"\\\\\"\n",
    "projectpath = os.getcwd() + \"\\\\..\\\\SpeakNSign\\\\\"\n",
    "# projectpath = os.getcwd() + \"\\\\..\\\\lookit\\\\\"\n",
    "\n",
    "# locations of videos and output\n",
    "videos_in = projectpath \n",
    "videos_out   = projectpath + \"out\"\n",
    "#videos_out = \"E:\\\\SpeakNSign\\\\out\"\n",
    "videos_out_openpose   = videos_out + \"\\\\openpose\"\n",
    "videos_out_timeseries = videos_out + \"\\\\timeseries\"\n",
    "videos_out_analyses   = videos_out + \"\\\\analyses\"\n",
    "\n",
    "print(videos_out_openpose)\n",
    "print(videos_out_timeseries)\n",
    "print(videos_out_analyses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Load the clean data as a DataFrame\n",
    "\n",
    "Reload the clean data file created in step 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve the list of base names of processed videos.\n",
    "try:\n",
    "    with open(videos_out + '\\\\clean.json') as json_file:\n",
    "        videos = json.load(json_file)\n",
    "        print(\"Existing clean.json found..\")\n",
    "except:\n",
    "    print(\"File clean.json not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('reading parquet file:')\n",
    "df = pq.read_table(videos_out_timeseries + '\\\\cleandata.parquet').to_pandas()\n",
    "\n",
    "#sort the column names as this helps with indexing\n",
    "df = df.sort_index(axis = 1)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we set all 0 values to as missing value `np.nan` to enable interpolation.\n",
    "Then use numpy's built in `interpolate` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(0.0, np.nan)\n",
    "\n",
    "#are we going to use all the data or a subset?\n",
    "first = 501\n",
    "last = 8500\n",
    "\n",
    "df = df.truncate(before  = first, after = last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dictionary of the subsets of OpenPose coordinates we want to average and then call `mean` on the Pandas dataframe. e.g.\n",
    "\n",
    "```\n",
    "meanpoints = {\n",
    "               \"headx\" : [0, 3, 45, 48, 51, 54],\n",
    "               \"heady\" : [1, 4, 46, 49, 52, 55],\n",
    "               \"allx\" :  [0, 3, 6, 9, ...],\n",
    "               \"ally\" :  [1, 4, 7, 10, ...]\n",
    "             }\n",
    "```\n",
    "\n",
    "Then we call the `vasc.averageCoordinateTimeSeries` function to average across sets of coordinates. For a given set of videos and people.\n",
    "\n",
    "In:\n",
    "```\n",
    "videos = \"All\"\n",
    "people = \"Both\"\n",
    "df2 = vasc.averageCoordinateTimeSeries(df,meanpoints,videos,people)\n",
    "df2.head\n",
    "```\n",
    "\n",
    "Out:\n",
    "```\n",
    "person      infant                                          parent   \n",
    "avgs         headx       heady          xs          ys       headx   \n",
    "501     565.996600  369.840600  534.895615  398.482538  471.686200   \n",
    "502     567.231800  369.887600  534.354198  398.706552  471.849400   \n",
    "503     567.228600  370.159600  534.444328  398.678133  471.711600   \n",
    "504     566.912600  369.857000  535.369536  398.551636  472.309400\n",
    "...            ...         ...         ...         ...         ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanpoints = {\"headx\" : vasc.headx,\n",
    "              \"heady\" : vasc.heady,\n",
    "              \"xs\" : vasc.xs, \n",
    "              \"ys\": vasc.ys}\n",
    "\n",
    "vids = \"All\"\n",
    "people = [\"infant\",\"parent\"]\n",
    "\n",
    "df2 = vasc.averageCoordinateTimeSeries(df,meanpoints,vids,people)\n",
    "\n",
    "df2.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data analysis\n",
    "\n",
    "We need to let Jupyter know where to find SyncPy and then import the functions we need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '..\\\\SyncPy\\\\src\\\\')   # To be able to import packages from parent directory\n",
    "sys.path.insert(0, '..\\\\SyncPy\\\\src\\\\Methods')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Import wanted module with every parent packages \"\"\"\n",
    "import Methods.DataFrom2Persons.Univariate.Continuous.Linear.Correlation as Correlation\n",
    "\n",
    "\"\"\" Import Utils modules \"\"\"\n",
    "from Methods.utils import Standardize\n",
    "from Methods.utils import ResampleAndInterpolate\n",
    "from Methods.utils.ExtractSignal import ExtractSignalFromCSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=np.arange(first,last+1)#number of samples\n",
    "\n",
    "\n",
    "\"\"\"Plot input signals\"\"\"\n",
    "plt.ion()\n",
    "f, axarr = plt.subplots(2, sharex=True)\n",
    "axarr[0].set_title('Infant')\n",
    "axarr[1].set_title('Parent')\n",
    "#axarr[0].set_xlabel('Frames')\n",
    "axarr[1].set_xlabel('Frames')\n",
    "\n",
    "vid = 'SS003'\n",
    "part = [\"infant\",\"parent\"]\n",
    "\n",
    "#to select a single column..\n",
    "#infant = df2[(vid, part[0], 'headx')]\n",
    "#parent = df2[(vid, part[1], 'headx')]\n",
    "\n",
    "#selecting multiple columns slightly messier\n",
    "infant = df2.loc[:,(vid, part[0], ('headx','heady'))]\n",
    "parent = df2.loc[:,(vid, part[1], ('headx','heady'))]\n",
    "\n",
    "axarr[0].plot(n,infant , label=\"i\")\n",
    "axarr[1].plot(n,parent, label=\"p\", color='b')\n",
    "axarr[0].legend(loc='best')\n",
    "axarr[1].legend(loc='best')\n",
    "\n",
    "\n",
    "plt.show() \n",
    "print(type(infant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Define class attributes of the wanted method \"\"\"\n",
    "\n",
    "tau_max = 999                       # the maximum lag at which correlation should be computed (in samples)\n",
    "plot=True                           # plot of the correlation fucntion\n",
    "standardization = True              # standardization of the time series to mean 0 and variance 1\n",
    "corr_tau_max = True                 # return of the maximum of correlation and its lag\n",
    "corr_coeff = True                   # computation of the correlation coefficient (Pearson's version)\n",
    "scale=True                          # scale factor to have correlaton in [-1,1]\n",
    "\n",
    "\"\"\" Instanciate the class with its attributes \"\"\"\n",
    "print(\"\\n\")\n",
    "\n",
    "try : \n",
    "    c=Correlation.Correlation(tau_max, plot, standardization, corr_tau_max, corr_coeff, scale)\n",
    "except TypeError as err :\n",
    "    print(\"TypeError in Correlation constructor : \\n\" + str(err))\n",
    "    sys.exit(-1)\n",
    "except ValueError as err :\n",
    "    print(\"ValueError in Correlation constructor : \\n\" + str(err))\n",
    "    sys.exit(-1)\n",
    "except Exception as e :\n",
    "    print(\"Exception in Correlation constructor : \\n\" + str(e))\n",
    "    sys.exit(-1)\n",
    "\n",
    "print(\"An instance the class is now created with the following parameters:\\n\" +\n",
    "      \"tau max = \" + str(tau_max) + \"\\n\" +\n",
    "      \"plot = \" + str(plot) + \"\\n\" +\n",
    "      \"standardization= \" + str(standardization) + \"\\n\" +\n",
    "      \"corr_tau_max = \" + str(corr_tau_max) + \"\\n\" +\n",
    "      \"corr_coeff =\" + str(corr_coeff) +\"\\n\" +\n",
    "      \"scale =\" + str(scale))\n",
    "\n",
    "\"\"\" Compute the method and get the result \"\"\"\n",
    "print(\"\\n\")\n",
    "print(\"Computing...\")\n",
    "\n",
    "\n",
    "try : \n",
    "    res= c.compute([infant, ])\n",
    "except TypeError as err :\n",
    "    print(\"TypeError in Correlation computation : \\n\" + str(err))\n",
    "    sys.exit(-1)\n",
    "except ValueError as err :\n",
    "    print(\"ValueError in Correlation computation : \\n\" + str(err))\n",
    "    sys.exit(-1)\n",
    "except Exception as e :\n",
    "    print(\"Exception in Correlation computation : \\n\" + str(e))\n",
    "    sys.exit(-1)\n",
    "\n",
    "\"\"\" Display result \"\"\"\n",
    "print(\"\\n\")\n",
    "print(\"**************************************** \\n\")\n",
    "print('Correlation complete result :')\n",
    "print(\"****************************************\\n\")\n",
    "print(\"Correlation function array:\")\n",
    "print(res['corr_funct'])\n",
    "print(\"Maximum value of the correlation %f and lag (in samples) %d:\" %(res['max_corr'],res['t_max']))\n",
    "print(\"Pearson's correlation coefficient %f:\" %(res['corr_coeff']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through colcuate for each pair\n",
    "for vid in videos:\n",
    "    infant = df2.loc[:,(vid, 'infant', ('headx','heady'))]\n",
    "    parent = df2.loc[:,(vid, 'parent', ('headx','heady'))]\n",
    "    try : \n",
    "        res= c.compute([infant, parent])\n",
    "    except TypeError as err :\n",
    "        print(\"TypeError in Correlation computation : \\n\" + str(err))\n",
    "        sys.exit(-1)\n",
    "    except ValueError as err :\n",
    "        print(\"ValueError in Correlation computation : \\n\" + str(err))\n",
    "        sys.exit(-1)\n",
    "    except Exception as e :\n",
    "        print(\"Exception in Correlation computation : \\n\" + str(e))\n",
    "        sys.exit(-1)\n",
    "        \n",
    "    print(vid)\n",
    "    print(\"Correlation function array:\")\n",
    "    print(res['corr_funct'])\n",
    "    print(\"Maximum value of the correlation %f and lag (in samples) %d:\" %(res['max_corr'],res['t_max']))\n",
    "    print(\"Pearson's correlation coefficient %f:\" %(res['corr_coeff']))\n",
    "    print(\"Correlation function array:\")\n",
    "    print(res['corr_funct'])\n",
    "    print(\"Maximum value of the correlation %f and lag (in samples) %d:\" %(res['max_corr'],res['t_max']))\n",
    "    print(\"Pearson's correlation coefficient %f:\" %(res['corr_coeff']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "https://towardsdatascience.com/granger-causality-and-vector-auto-regressive-model-for-time-series-forecasting-3226a64889a6\n",
    "\n",
    "https://www.machinelearningplus.com/time-series/time-series-analysis-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
