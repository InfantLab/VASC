{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Actor Synchroncy and Causality (VASC)\n",
    "## RAEng: Measuring Responsive Caregiving Project\n",
    "### Caspar Addyman, 2020\n",
    "### https://github.com/infantlab/VASC\n",
    "\n",
    "# Step 3: Analyse the data using SyncPy\n",
    "\n",
    "This script uses output from  human figure recognition neural network to create labeled wireframes for each figure in each frame of a video. \n",
    "In this step we start with a clean numpy array of all [OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) data from all pairs of individuals. We then use [SyncPy](https://github.com/syncpy/SyncPy) for data analysis. SyncPy was developed by Giovanna Varni, Mohamed Chetouani and colleagues at the Institut des Systèmes Intelligentes et Robotique (ISIR) at the Université Pierre et Marie Curie (UPMC), Paris 6, France.  \n",
    "\n",
    "A technical paper is found\n",
    "\n",
    "A \n",
    "\n",
    "\n",
    "NOTE:\n",
    "At present \n",
    "https://github.com/InfantLab/SyncPy\n",
    "\n",
    "\n",
    "https://towardsdatascience.com/granger-causality-and-vector-auto-regressive-model-for-time-series-forecasting-3226a64889a6\n",
    "\n",
    "https://www.machinelearningplus.com/time-series/time-series-analysis-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np       \n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import logging\n",
    "import ipywidgets as widgets  #let's us add buttons and sliders to this page.\n",
    "from ipycanvas import Canvas\n",
    "\n",
    "import vasc #a module of our own functions (found in vasc.py in this folder)\n",
    "\n",
    "#turn on debugging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "%pdb on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jupwd =  os.getcwd() + \"\\\\\"\n",
    "#projectpath = os.getcwd() + \"\\\\..\\\\SpeakNSign\\\\\"\n",
    "projectpath = os.getcwd() + \"\\\\..\\\\lookit\\\\\"\n",
    "\n",
    "# locations of videos and output\n",
    "videos_in = projectpath \n",
    "videos_out   = projectpath + \"out\"\n",
    "#videos_out = \"E:\\\\SpeakNSign\\\\out\"\n",
    "videos_out_openpose   = videos_out + \"\\\\openpose\"\n",
    "videos_out_timeseries = videos_out + \"\\\\timeseries\"\n",
    "videos_out_analyses   = videos_out + \"\\\\analyses\"\n",
    "\n",
    "print(videos_out_openpose)\n",
    "print(videos_out_timeseries)\n",
    "print(videos_out_analyses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Load the clean data as a DataFrame\n",
    "\n",
    "Reload the clean data file created in step 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve the list of base names of processed videos.\n",
    "with open(videos_out + '\\\\clean.json') as json_file:\n",
    "    videos = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can reload the values without recomputing\n",
    "reloaded = np.load(videos_out_timeseries + '\\\\cleandata.npz')\n",
    "keypoints_array = reloaded[\"keypoints_array\"] #the unprocessed data\n",
    "keypoints_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pqdf = pq.read_table(videos_out_timeseries + '\\\\cleandata.parquet').to_pandas()\n",
    "print(pqdf.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '..\\\\SyncPy\\\\src\\\\')   # To be able to import packages from parent directory\n",
    "sys.path.insert(0, '..\\\\SyncPy\\\\src\\\\Methods')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"\\n\")\n",
    "print(\"************************************************************************************************\")\n",
    "print(\"This scripts computes the correlation between two monovariate signals.\"\n",
    "       \"First input is a sinewave of 1 Hz frequency, the second one\\n is the sum of this sinewave\"\n",
    "       \"with a gaussian random process having zero mean and unitary\\n variance.\")\n",
    "print(\"************************************************************************************************\")\n",
    "\n",
    "\"\"\" Import wanted module with every parent packages \"\"\"\n",
    "import Methods.DataFrom2Persons.Univariate.Continuous.Linear.Correlation as Correlation\n",
    "\n",
    "\"\"\" Import Utils modules \"\"\"\n",
    "from Methods.utils import Standardize\n",
    "from Methods.utils.ExtractSignal import ExtractSignalFromCSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Define signals in pd.dataFrame format \"\"\"\n",
    "\n",
    "#Define parameters\n",
    "N=1024 # number of samples\n",
    "f=1.0  # sinewave frequency (Hz)\n",
    "Fs=200 # sampling frequency (Hz)\n",
    "\n",
    "n=np.arange(0,N)#number of samples\n",
    "\n",
    "# Create signals\n",
    "x = pd.DataFrame({'X':np.sin(2*3.14*f*n/Fs)}, np.arange(0,N))\n",
    "y = pd.DataFrame({'Y':np.sin(2*3.14*f*n/Fs)+10*np.random.randn(1,N)[0]},np.arange(0,N))\n",
    "\n",
    "\"\"\"Plot input signals\"\"\"\n",
    "plt.ion()\n",
    "f, axarr = plt.subplots(2, sharex=True)\n",
    "axarr[0].set_title('Input signals')\n",
    "axarr[0].set_xlabel('Samples')\n",
    "axarr[1].set_xlabel('Samples')\n",
    "\n",
    "axarr[0].plot(n, x, label=\"x\")\n",
    "axarr[1].plot(n, y, label=\"y\", color='r')\n",
    "axarr[0].legend(loc='best')\n",
    "axarr[1].legend(loc='best')\n",
    "\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Define class attributes of the wanted method \"\"\"\n",
    "\n",
    "tau_max = 999                       # the maximum lag at which correlation should be computed (in samples)\n",
    "plot=True                           # plot of the correlation fucntion\n",
    "standardization = True              # standardization of the time series to mean 0 and variance 1\n",
    "corr_tau_max = True                 # return of the maximum of correlation and its lag\n",
    "corr_coeff = True                   # computation of the correlation coefficient (Pearson's version)\n",
    "scale=True                          # scale factor to have correlaton in [-1,1]\n",
    "\n",
    "\"\"\" Instanciate the class with its attributes \"\"\"\n",
    "print(\"\\n\")\n",
    "\n",
    "try : \n",
    "    c=Correlation.Correlation(tau_max, plot, standardization, corr_tau_max, corr_coeff, scale)\n",
    "except TypeError as err :\n",
    "    print(\"TypeError in Correlation constructor : \\n\" + str(err))\n",
    "    sys.exit(-1)\n",
    "except ValueError as err :\n",
    "    print(\"ValueError in Correlation constructor : \\n\" + str(err))\n",
    "    sys.exit(-1)\n",
    "except Exception as e :\n",
    "    print(\"Exception in Correlation constructor : \\n\" + str(e))\n",
    "    sys.exit(-1)\n",
    "\n",
    "print(\"An instance the class is now created with the following parameters:\\n\" +\n",
    "      \"tau max = \" + str(tau_max) + \"\\n\" +\n",
    "      \"plot = \" + str(plot) + \"\\n\" +\n",
    "      \"standardization= \" + str(standardization) + \"\\n\" +\n",
    "      \"corr_tau_max = \" + str(corr_tau_max) + \"\\n\" +\n",
    "      \"corr_coeff =\" + str(corr_coeff) +\"\\n\" +\n",
    "      \"scale =\" + str(scale))\n",
    "\n",
    "\"\"\" Compute the method and get the result \"\"\"\n",
    "print(\"\\n\")\n",
    "print(\"Computing...\")\n",
    "\n",
    "try : \n",
    "    res= c.compute([x, y])\n",
    "except TypeError as err :\n",
    "    print(\"TypeError in Correlation computation : \\n\" + str(err))\n",
    "    sys.exit(-1)\n",
    "except ValueError as err :\n",
    "    print(\"ValueError in Correlation computation : \\n\" + str(err))\n",
    "    sys.exit(-1)\n",
    "except Exception as e :\n",
    "    print(\"Exception in Correlation computation : \\n\" + str(e))\n",
    "    sys.exit(-1)\n",
    "\n",
    "\"\"\" Display result \"\"\"\n",
    "print(\"\\n\")\n",
    "print(\"**************************************** \\n\")\n",
    "print('Correlation complete result :')\n",
    "print(\"****************************************\\n\")\n",
    "print(\"Correlation function array:\")\n",
    "print(res['corr_funct'])\n",
    "print(\"Maximum value of the correlation %f and lag (in samples) %d:\" %(res['max_corr'],res['t_max']))\n",
    "print(\"Pearson's correlation coefficient %f:\" %(res['corr_coeff']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
